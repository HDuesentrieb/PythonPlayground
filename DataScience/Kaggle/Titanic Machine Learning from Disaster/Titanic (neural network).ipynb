{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load training data\n",
    "train_raw = pandas.read_csv('train.csv', sep=',', index_col=0)\n",
    "\n",
    "#load test data (with answer)\n",
    "test_raw = pandas.read_csv('test.csv', sep=',', index_col=0)\n",
    "gender_sub = pandas.read_csv('gender_submission.csv', sep=',', index_col=0)\n",
    "\n",
    "data_len = len(train_raw)\n",
    "itest_len = data_len // 10\n",
    "valid_len = data_len // 10 * 2\n",
    "train_len = data_len - valid_len - itest_len\n",
    "\n",
    "test_len = len(test_raw)\n",
    "\n",
    "def from_panda_to_numpy(df, \n",
    "                        age_min, age_max, \n",
    "                        sibsp_min, sibsp_max, \n",
    "                        parch_min, parch_max,\n",
    "                        fare_min, fare_max ):\n",
    "    df_len = len(df)\n",
    "\n",
    "    def convert_name_to_label(names_):\n",
    "        lower_name = names_.str.lower()\n",
    "        result = np.zeros(len(names_))\n",
    "        result[lower_name.str.contains('mrs.')] = 1\n",
    "        result[lower_name.str.contains('mr.')] = 2\n",
    "        result[lower_name.str.contains('ms.')] = 3\n",
    "        result[lower_name.str.contains('mlle.')] = 4\n",
    "        result[lower_name.str.contains('miss.')] = 5\n",
    "        result[lower_name.str.contains('sir.')] = 6\n",
    "        result[lower_name.str.contains('rev.')] = 7\n",
    "        result[lower_name.str.contains('mme.')] = 8\n",
    "        result[lower_name.str.contains('master.')] = 9\n",
    "        result[lower_name.str.contains('major.')] = 10\n",
    "        result[lower_name.str.contains('lady.')] = 11\n",
    "        result[lower_name.str.contains('jonkheer.')] = 12\n",
    "        result[lower_name.str.contains('dr.')] = 13\n",
    "        result[lower_name.str.contains('don.')] = 14\n",
    "        result[lower_name.str.contains('col.')] = 15\n",
    "        result[lower_name.str.contains('capt.')] = 16\n",
    "        result[lower_name.str.contains('countess.')] = 17\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def convert_sex_to_label(sex_):\n",
    "        lower_sex = sex_.str.lower()\n",
    "        result = np.zeros(len(sex_))\n",
    "        result[lower_sex == 'male'] = 0\n",
    "        result[lower_sex == 'female'] = 1\n",
    "\n",
    "        return result\n",
    "\n",
    "    def convert_cabin_to_label(cabin_):\n",
    "        lower_cabin = cabin_.str.lower()\n",
    "        result = np.zeros(len(cabin_))\n",
    "        lower_cabin = lower_cabin.replace(np.nan, '', regex=True)\n",
    "        result[lower_cabin.str.contains('a')] = 1\n",
    "        result[lower_cabin.str.contains('b')] = 2\n",
    "        result[lower_cabin.str.contains('c')] = 3\n",
    "        result[lower_cabin.str.contains('d')] = 4\n",
    "        result[lower_cabin.str.contains('e')] = 5\n",
    "        result[lower_cabin.str.contains('f')] = 6\n",
    "        result[lower_cabin.str.contains('g')] = 7\n",
    "        result[lower_cabin.str.contains('t')] = 8\n",
    "\n",
    "        return result\n",
    "\n",
    "    def convert_embark_to_label(embark_):\n",
    "        lower_embark = embark_.str.lower()\n",
    "        result = np.zeros(len(cabin_))\n",
    "        result[lower_embark.str.contains('c')] = 1\n",
    "        result[lower_embark.str.contains('q')] = 2\n",
    "        result[lower_embark.str.contains('s')] = 3\n",
    "        \n",
    "    def clean_up_age(age_):\n",
    "        result = np.zeros(len(age_), dtype=np.float32)\n",
    "        result[:] = age_\n",
    "        result[np.isnan(result)] = 0\n",
    "        result[result < 1] *= 100 \n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def clean_up_fare(fare_):\n",
    "        result = np.zeros(len(fare_), dtype=np.float32)\n",
    "        result[:] = fare_\n",
    "        result[np.isnan(result)] = 0\n",
    "        \n",
    "        return result\n",
    "        \n",
    "        \n",
    "    full_dataset = np.zeros((df_len, 32), dtype=np.float32)\n",
    "    full_dataset[:, 0] = df[\"Pclass\"] / 3\n",
    "    salutaion_ = convert_name_to_label(df[\"Name\"])\n",
    "    full_dataset[:, 1] = salutaion_ == 1\n",
    "    full_dataset[:, 2] = salutaion_ == 2\n",
    "    full_dataset[:, 3] = salutaion_ == 3\n",
    "    full_dataset[:, 4] = salutaion_ == 4\n",
    "    full_dataset[:, 5] = salutaion_ == 5\n",
    "    full_dataset[:, 6] = salutaion_ == 6\n",
    "    full_dataset[:, 7] = salutaion_ == 7\n",
    "    full_dataset[:, 8] = salutaion_ == 8\n",
    "    full_dataset[:, 9] = salutaion_ == 9\n",
    "    full_dataset[:, 10] = salutaion_ == 10\n",
    "    full_dataset[:, 11] = salutaion_ == 11\n",
    "    full_dataset[:, 12] = salutaion_ == 12\n",
    "    full_dataset[:, 13] = salutaion_ == 13\n",
    "    full_dataset[:, 14] = salutaion_ == 14\n",
    "    full_dataset[:, 15] = salutaion_ == 15\n",
    "    full_dataset[:, 16] = salutaion_ == 16\n",
    "    full_dataset[:, 17] = salutaion_ == 17\n",
    "    full_dataset[:, 18] = convert_sex_to_label(df[\"Sex\"])\n",
    "    full_dataset[:, 19] = (clean_up_age(df[\"Age\"]) - age_min) / (age_max - age_min)\n",
    "    full_dataset[:, 20] = (df[\"SibSp\"] - sibsp_min) / (sibsp_max - sibsp_min)\n",
    "    full_dataset[:, 21] = (df[\"Parch\"] - parch_min) / (parch_max - parch_min)\n",
    "    full_dataset[:, 22] = (clean_up_fare(df[\"Fare\"]) - fare_min) / (fare_max - fare_min)\n",
    "    carbin_type = convert_cabin_to_label(df[\"Cabin\"])\n",
    "    full_dataset[:, 23] = carbin_type == 0\n",
    "    full_dataset[:, 24] = carbin_type == 1\n",
    "    full_dataset[:, 25] = carbin_type == 2\n",
    "    full_dataset[:, 26] = carbin_type == 3\n",
    "    full_dataset[:, 27] = carbin_type == 4\n",
    "    full_dataset[:, 28] = carbin_type == 5\n",
    "    full_dataset[:, 29] = carbin_type == 6\n",
    "    full_dataset[:, 30] = carbin_type == 7\n",
    "    full_dataset[:, 31] = carbin_type == 8\n",
    "    \n",
    "    return full_dataset\n",
    "\n",
    "age_min_ = min(0, train_raw[\"Age\"].min())\n",
    "age_max_ = train_raw[\"Age\"].max()\n",
    "sibsp_min_ = train_raw[\"SibSp\"].min()\n",
    "sibsp_max_ = train_raw[\"SibSp\"].max()\n",
    "parch_min_ = train_raw[\"Parch\"].min()\n",
    "parch_max_ = train_raw[\"Parch\"].max()\n",
    "fare_min_ = train_raw[\"Fare\"].min()\n",
    "fare_max_ = train_raw[\"Fare\"].max()\n",
    "    \n",
    "full_dataset = from_panda_to_numpy(train_raw, \n",
    "                                   age_min_, age_max_, sibsp_min_, sibsp_max_, parch_min_, parch_max_, fare_min_, fare_max_)\n",
    "full_label = np.zeros((data_len, 2), dtype=np.float32)\n",
    "full_label[:, 0] = train_raw[\"Survived\"]\n",
    "full_label[:, 1] = 1-full_label[:, 0] \n",
    "\n",
    "test_dataset = from_panda_to_numpy(test_raw,\n",
    "                                   age_min_, age_max_, sibsp_min_, sibsp_max_, parch_min_, parch_max_, fare_min_, fare_max_)\n",
    "test_labels = np.zeros((test_len, 2), dtype=np.float32)\n",
    "test_labels[:, 0] = gender_sub[\"Survived\"]\n",
    "test_labels[:, 1] = 1-test_labels[:, 0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#utility function\n",
    "def get_train_valid_set(full_dataset, full_label):\n",
    "    rand_perm = np.random.permutation(len(full_dataset))\n",
    "    dataset = full_dataset[rand_perm]\n",
    "    label = full_label[rand_perm]\n",
    "    return dataset[0:train_len], label[0:train_len] , \\\n",
    "                dataset[train_len:train_len+valid_len], label[train_len:train_len+valid_len], \\\n",
    "                dataset[train_len+valid_len:], label[train_len+valid_len:]\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "train_dataset, train_labels, valid_dataset, valid_labels, itest_dataset, itest_labels \\\n",
    "    = get_train_valid_set(full_dataset, full_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_layer_size_1=1024\n",
    "hidden_layer_size_2=300\n",
    "hidden_layer_size_3=50\n",
    "input_size = 32\n",
    "dropout = 0.7\n",
    "num_labels = 2\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.constant(train_dataset)\n",
    "    tf_train_labels = tf.constant(train_labels)\n",
    "    tf_lambda = tf.placeholder(tf.float32)\n",
    "    tf_dropout = tf.placeholder(tf.float32)\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_itest_dataset = tf.constant(itest_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    W1 = tf.Variable(\n",
    "        tf.truncated_normal([input_size, hidden_layer_size_1], stddev=0.1))\n",
    "    b1 = tf.Variable(tf.zeros([hidden_layer_size_1]))\n",
    "    W2 = tf.Variable(\n",
    "        tf.truncated_normal([hidden_layer_size_1, hidden_layer_size_2], stddev=0.1))\n",
    "    b2 = tf.Variable(tf.zeros([hidden_layer_size_2]))\n",
    "    W3 = tf.Variable(\n",
    "        tf.truncated_normal([hidden_layer_size_2, hidden_layer_size_3], stddev=0.1))\n",
    "    b3 = tf.Variable(tf.zeros([hidden_layer_size_3]))\n",
    "    W4 = tf.Variable(\n",
    "        tf.truncated_normal([hidden_layer_size_3, num_labels], stddev=0.1))\n",
    "    b4 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    a1 = tf.nn.relu(tf.matmul(tf_train_dataset, W1) + b1)\n",
    "    a2 = tf.nn.relu(tf.matmul(tf.nn.dropout(a1, tf_dropout), W2) + b2)\n",
    "    a3 = tf.nn.relu(tf.matmul(tf.nn.dropout(a2, tf_dropout), W3) + b3)\n",
    "    logits = tf.matmul(tf.nn.dropout(a3, tf_dropout), W4) + b4\n",
    "    \n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    loss += tf_lambda * (tf.nn.l2_loss(W1) + tf.nn.l2_loss(W2) + tf.nn.l2_loss(W3) + tf.nn.l2_loss(W4))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    valid_a1 = tf.nn.relu(tf.matmul(tf_valid_dataset, W1) + b1)\n",
    "    valid_a2 = tf.nn.relu(tf.matmul(valid_a1, W2) + b2)\n",
    "    valid_a3 = tf.nn.relu(tf.matmul(valid_a2, W3) + b3)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(valid_a3, W4) + b4)\n",
    "    \n",
    "    test_a1 = tf.nn.relu(tf.matmul(tf_test_dataset, W1) + b1)\n",
    "    test_a2 = tf.nn.relu(tf.matmul(test_a1, W2) + b2)\n",
    "    test_a3 = tf.nn.relu(tf.matmul(test_a2, W3) + b3)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(test_a3, W4) + b4)\n",
    "    \n",
    "    #internal test set\n",
    "    itest_a1 = tf.nn.relu(tf.matmul(tf_itest_dataset, W1) + b1)\n",
    "    itest_a2 = tf.nn.relu(tf.matmul(itest_a1, W2) + b2)\n",
    "    itest_a3 = tf.nn.relu(tf.matmul(itest_a2, W3) + b3)\n",
    "    itest_prediction = tf.nn.softmax(tf.matmul(itest_a3, W4) + b4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized with lambda 0.001000 dropout 0.500000\n",
      "Train loss at step 0: 2.096198\n",
      "Train accuracy: 48.7%\n",
      "Validation accuracy: 61.2%\n",
      "Train loss at step 500: 1.167129\n",
      "Train accuracy: 86.4%\n",
      "Validation accuracy: 78.1%\n",
      "Train loss at step 1000: 0.841199\n",
      "Train accuracy: 88.1%\n",
      "Validation accuracy: 76.4%\n",
      "Train loss at step 1500: 0.622628\n",
      "Train accuracy: 88.5%\n",
      "Validation accuracy: 76.4%\n",
      "Train loss at step 2000: 0.528839\n",
      "Train accuracy: 88.3%\n",
      "Validation accuracy: 76.4%\n",
      "Train loss at step 2500: 0.427473\n",
      "Train accuracy: 89.3%\n",
      "Validation accuracy: 77.0%\n",
      "Train loss at step 3000: 0.376689\n",
      "Train accuracy: 90.5%\n",
      "Validation accuracy: 77.0%\n",
      "Train accuracy: 90.5%\n",
      "Cross validation accuracy: 77.0%\n",
      "Test accuracy: 86.5%\n",
      "===== Model scoring =====\n"
     ]
    }
   ],
   "source": [
    "assert not np.any(np.isnan(train_dataset))\n",
    "assert not np.any(np.isnan(valid_dataset))\n",
    "assert not np.any(np.isnan(itest_dataset))\n",
    "assert not np.any(np.isnan(train_labels))\n",
    "assert not np.any(np.isnan(valid_labels))\n",
    "assert not np.any(np.isnan(itest_labels))\n",
    "\n",
    "assert not np.any(np.isnan(test_dataset))\n",
    "\n",
    "\n",
    "lambdas = [0.001]\n",
    "dropsout = [0.5]\n",
    "\n",
    "num_steps = 3001\n",
    "\n",
    "error_trains = np.zeros((len(lambdas), len(dropsout)))\n",
    "error_vals = np.zeros((len(lambdas), len(dropsout)))\n",
    "error_test = np.zeros((len(lambdas), len(dropsout)))\n",
    "\n",
    "for i, lambda_t in enumerate(lambdas) :\n",
    "    for j, dropout in enumerate(dropsout):\n",
    "\n",
    "        with tf.Session(graph=graph) as session:\n",
    "            tf.global_variables_initializer().run()\n",
    "            print(\"Initialized with lambda %f dropout %f\" % (lambda_t, dropout))\n",
    "            for step in range(num_steps):\n",
    "                # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "                # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "                # and the value is the numpy array to feed to it.\n",
    "                feed_dict = {tf_lambda : lambda_t, tf_dropout : dropout}\n",
    "                _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "                if (step % 500 == 0):\n",
    "                    print(\"Train loss at step %d: %f\" % (step, l))\n",
    "                    print(\"Train accuracy: %.1f%%\" % accuracy(predictions, train_labels))\n",
    "                    print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                        valid_prediction.eval(), valid_labels))\n",
    "\n",
    "            print(\"Train accuracy: %.1f%%\" % accuracy(predictions, train_labels))\n",
    "            print(\"Cross validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "            print(\"Test accuracy: %.1f%%\" % accuracy(itest_prediction.eval(), itest_labels))\n",
    "\n",
    "            print(\"===== Model scoring =====\")\n",
    "            result = test_prediction.eval()\n",
    "\n",
    "            error_trains[i, j] = 100-accuracy(train_prediction.eval(feed_dict), train_labels)\n",
    "            error_vals[i, j] = 100-accuracy(valid_prediction.eval(feed_dict), valid_labels)\n",
    "            error_test[i, j] = 100-accuracy(itest_prediction.eval(feed_dict), itest_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#output dataset for submission\n",
    "submission_df = pandas.DataFrame(index=gender_sub.index, columns=[\"Survived\"])\n",
    "submission_df[\"Survived\"] = 1- np.argmax(result, 1) #Survive is marked as label 0.\n",
    "submission_df.to_csv(\"submission.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The best result this model can produce is at max 75%\n",
    "\n",
    "Your submission scored 0.75598, which is not an improvement of your best score. Keep trying!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
