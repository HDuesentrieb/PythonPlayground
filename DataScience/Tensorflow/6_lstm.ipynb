{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "            'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        name = f.namelist()[0]\n",
    "        data = tf.compat.as_str(f.read(name))\n",
    "    return data\n",
    "    \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    else:\n",
    "        print('Unexpected character: %s' % char)\n",
    "        return 0\n",
    "    \n",
    "def id2char(dictid):\n",
    "    if dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "    \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "    \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "def random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "    return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Parameters:\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                                                         \n",
    "    cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]    # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.297980 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.06\n",
      "================================================================================\n",
      "yanaomthiqhefn crtda ycersfugsjfutneiyiws gltmkrlvtppk xza an slpmvyd xqxeayapt \n",
      "oeiolmdivswqsriw  pbhm viqlmaemie rmeotendrtujkowsuyctmwlpve qriwn o x k bw  hs \n",
      " z  pfdatazancmriuy mhb ztsj   ilpcox apcfgf cicnelezaceeobmalyyjt tseiq okhbcmx\n",
      "g lwisboxtqne  v mhtiksng yesm qa nauako mmctyg lywde athf tr mhercsli feqvcismm\n",
      "mgllpirqwiej of gv i cme eepqj vsefad hitlmkeazuinjqigu siirthajeyvj fxahnrike a\n",
      "================================================================================\n",
      "Validation set perplexity: 19.99\n",
      "Average loss at step 100: 2.592564 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.71\n",
      "Validation set perplexity: 10.10\n",
      "Average loss at step 200: 2.241147 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.46\n",
      "Validation set perplexity: 8.49\n",
      "Average loss at step 300: 2.099373 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.54\n",
      "Validation set perplexity: 8.09\n",
      "Average loss at step 400: 2.001798 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.65\n",
      "Validation set perplexity: 7.96\n",
      "Average loss at step 500: 1.938446 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.58\n",
      "Validation set perplexity: 7.20\n",
      "Average loss at step 600: 1.909201 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.27\n",
      "Validation set perplexity: 7.22\n",
      "Average loss at step 700: 1.862056 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.50\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 800: 1.821154 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 6.53\n",
      "Average loss at step 900: 1.827481 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.08\n",
      "Validation set perplexity: 6.37\n",
      "Average loss at step 1000: 1.827390 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "================================================================================\n",
      "eved cicro ae work squtiry wherment of contimle in grounuced infelang nact wide \n",
      "ancle the scome imporring avantar clourde hiter one nine zero eight poter or jes\n",
      "veing rewerred a a has from warly i fere maditon be wase a severan s splokd tere\n",
      "zering heplich three forr of comgouse sory as a pucter van a man colluge is deco\n",
      "s doince of peover compent one nine four zero heubal was infons the cavedy in on\n",
      "================================================================================\n",
      "Validation set perplexity: 6.27\n",
      "Average loss at step 1100: 1.778262 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 5.85\n",
      "Average loss at step 1200: 1.750519 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 5.74\n",
      "Average loss at step 1300: 1.733317 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 5.59\n",
      "Average loss at step 1400: 1.743538 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 5.70\n",
      "Average loss at step 1500: 1.734826 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 5.49\n",
      "Average loss at step 1600: 1.747088 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 5.67\n",
      "Average loss at step 1700: 1.711688 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 5.44\n",
      "Average loss at step 1800: 1.671354 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 1900: 1.647397 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 2000: 1.694339 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "================================================================================\n",
      "xitwern neading surveny trut jptter withles line camitenting producaing anlicary\n",
      "m desponds mystions his cainsian tmy the cousc leed was yabansk stircal the vela\n",
      "menting drunghting towtardell audracts s x rrobll driend scym six fourson of ing\n",
      "uct her centrall reveld lated some or resumes gizs word uncurations worded kusin\n",
      "ums oldies three two jours x graws jankess of nominia becumpol ennoture script r\n",
      "================================================================================\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 2100: 1.685590 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 2200: 1.681815 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.41\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 2300: 1.639388 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 2400: 1.658806 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 2500: 1.678803 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 2600: 1.653952 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 2700: 1.657741 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 2800: 1.650392 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 2900: 1.650701 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 3000: 1.654599 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "================================================================================\n",
      "jagion and states of the buth itpand no connect s martiin became lo indian are p\n",
      "pyctiblismard to particlism cotion four tsom in p five scennemet in india comput\n",
      "weckould such as becyoral usize deax station of which from the jarentant cownden\n",
      "cis poodetry againter call aupuncer to the liarise refer balled of calitnced the\n",
      "qued corsal becaued all was the matwes a sm after and the lappus it have altersa\n",
      "================================================================================\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 3100: 1.626944 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 3200: 1.647239 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 3300: 1.641850 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 3400: 1.670302 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 3500: 1.658068 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 3600: 1.668760 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 3700: 1.645141 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 3800: 1.649218 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 3900: 1.638174 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 4000: 1.653718 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.69\n",
      "================================================================================\n",
      "ing the such for states sevending a dincedetity in boind the action entertation \n",
      "gnesfor caid an madawa for oftity carrox for oqued wead in his the culking or pr\n",
      "he it hoke history stors in opended usence for o tembilted on micsop febliew of \n",
      " been the constit difints of remain of heres in the division enn be whed two fou\n",
      "us by seef yleter of the fatimutions for the live datived own and the plates fly\n",
      "================================================================================\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 4100: 1.631862 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 4200: 1.635610 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 4300: 1.612112 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 4400: 1.613518 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.42\n",
      "Average loss at step 4500: 1.616899 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 4600: 1.616210 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 4700: 1.626453 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 4800: 1.632648 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 4900: 1.634618 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 5000: 1.612095 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "================================================================================\n",
      "le signis in electronise suckeasis indisticated of chie s case unilie monchan ca\n",
      "le kan outsed to as bassic vat is gresch cossists microin contraurges engud act \n",
      "uted a grelit wiqa troush jeeguor s went one six is zero nine othoen one seven n\n",
      "xingly the maciater and and telp i armed alsan a codrectes ameitt one nine five \n",
      "gabor six intersicans of ans the semidal a isaze in activy in that one seven zer\n",
      "================================================================================\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 5100: 1.608178 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 5200: 1.588755 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 5300: 1.572136 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 5400: 1.577572 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 5500: 1.570297 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5600: 1.580860 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5700: 1.570257 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 5800: 1.579757 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 5900: 1.574669 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 6000: 1.544858 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "================================================================================\n",
      "ficd meborists and gending also our lats calso from pirst economic occums articl\n",
      " which one nine eight brule of schoom and bivners frame engraming plaininish in \n",
      "y and latin and not of begis a her of areurizal to histokon distifids in offocke\n",
      "g of goldent ielmabims joyhid the herati eride of the mueter one mivines habit c\n",
      "x and works named so virding prebain which consixtcter hears imported when ones \n",
      "================================================================================\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6100: 1.565062 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6200: 1.537067 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6300: 1.544731 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6400: 1.539156 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 6500: 1.557105 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6600: 1.595269 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6700: 1.581250 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6800: 1.603856 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6900: 1.580888 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 7000: 1.578468 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "================================================================================\n",
      "will daymest whotroy degites of the scleking a herically in most german evalawia\n",
      "tactiring by for deligion annesing after calizg of the generents was one zero ze\n",
      "nacland at the kybus both of and the afriizra of great with pacbodually at at th\n",
      "k bist in which b ustico there the bocker impoces includewown performated by aga\n",
      "ventt from auropticed ital s cods aquirait faymation off one eight nine about th\n",
      "================================================================================\n",
      "Validation set perplexity: 4.24\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run(\n",
    "            [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 0\n",
    "\n",
    "Visualize the graph and allow you to debug the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = \"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph_1 = tf.Graph()\n",
    "with graph_1.as_default():\n",
    "    \n",
    "    # Parameters: input, previous output and bias    \n",
    "    ifcox = tf.Variable(tf.truncated_normal([vocabulary_size, 4 * num_nodes], -0.1, 0.1))\n",
    "    ifcom = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "    ifcob = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "    \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        all_gate = tf.matmul(i, ifcox) + tf.matmul(o, ifcom) + ifcob\n",
    "        input_gate = tf.sigmoid(all_gate[:, 0: num_nodes])\n",
    "        forget_gate = tf.sigmoid(all_gate[:, num_nodes: 2* num_nodes])\n",
    "        update = all_gate[:, 2*num_nodes: 3* num_nodes]\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(all_gate[:, 3*num_nodes:])\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]    # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.294123 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.95\n",
      "================================================================================\n",
      "iun viz orhh he js  ycibntveelap gnmtz fzo r ddpvwoqt zjehesdeorhiwcoljtlly  a n\n",
      "breabug ai scnn r ars ach bchtdj w hrnu fdemxte akvgem  zqobgxtbl btudcsteqmtfta\n",
      "gu pcaseane nulhknjebohxw  ffwho   ttlqep iro ieeabqkh ccfhudoy  ui hev ri o rwx\n",
      "yznhiaygnzob dkdnkwi t eqjfc nekl gzeflch nekiudkodrakb omceqldndkeo na oughs p \n",
      "fnitfc  firwhrvclogcbsyecb rpnkeliote  y dch thmsaush j   qiry plidxhma  i  umv \n",
      "================================================================================\n",
      "Validation set perplexity: 19.83\n",
      "Average loss at step 100: 2.582345 learning rate: 10.000000\n",
      "Minibatch perplexity: 12.59\n",
      "Validation set perplexity: 10.95\n",
      "Average loss at step 200: 2.234396 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.38\n",
      "Validation set perplexity: 8.87\n",
      "Average loss at step 300: 2.071583 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.86\n",
      "Validation set perplexity: 7.86\n",
      "Average loss at step 400: 1.985008 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.74\n",
      "Validation set perplexity: 7.64\n",
      "Average loss at step 500: 1.989757 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.57\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 600: 1.914515 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.32\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 700: 1.886122 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.74\n",
      "Validation set perplexity: 6.71\n",
      "Average loss at step 800: 1.861737 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.72\n",
      "Validation set perplexity: 6.39\n",
      "Average loss at step 900: 1.852095 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 6.29\n",
      "Average loss at step 1000: 1.785463 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.98\n",
      "================================================================================\n",
      "dopice termition the ponater in recuenealenglasing fugections repelther the eigh\n",
      "albon it peosted and inthaids anshain yore that mather capherogen repeenty and i\n",
      "feted the codfited bs a demed the five tto zero zero zero zero hirfered is depia\n",
      "ppaykid as in lavioy of thine syee of sampicual wor subeose of duevers of its co\n",
      "bers spate aslacal fidmally and gelition to one nine nine seven zero zero zero z\n",
      "================================================================================\n",
      "Validation set perplexity: 6.13\n",
      "Average loss at step 1100: 1.757123 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 6.14\n",
      "Average loss at step 1200: 1.787911 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.22\n",
      "Validation set perplexity: 6.01\n",
      "Average loss at step 1300: 1.769615 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 5.85\n",
      "Average loss at step 1400: 1.741623 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 5.65\n",
      "Average loss at step 1500: 1.728993 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.29\n",
      "Validation set perplexity: 5.51\n",
      "Average loss at step 1600: 1.716257 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 5.45\n",
      "Average loss at step 1700: 1.741153 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.17\n",
      "Validation set perplexity: 5.51\n",
      "Average loss at step 1800: 1.704012 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 5.37\n",
      "Average loss at step 1900: 1.709417 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 2000: 1.719799 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.93\n",
      "================================================================================\n",
      "ming bests propesing two deaddalle it cumon as conqueria linatecters it fumbelti\n",
      "ve sportaly manalused swarmentenaly flamilattly nomena and vervek the ded precor\n",
      "ussts a cometicularoyic conklesedais chanded proter warly a known cares it no ma\n",
      "bern texps of the bring and to lite and abill of see georiting moallited yount m\n",
      "f as polleve dos tran in s it two th best mazitalese links gene age would nugh e\n",
      "================================================================================\n",
      "Validation set perplexity: 5.37\n",
      "Average loss at step 2100: 1.700444 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 5.49\n",
      "Average loss at step 2200: 1.673469 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 5.40\n",
      "Average loss at step 2300: 1.680991 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 5.36\n",
      "Average loss at step 2400: 1.680357 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 5.37\n",
      "Average loss at step 2500: 1.699464 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 2600: 1.673645 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 2700: 1.689015 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 2800: 1.651805 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 2900: 1.650199 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 3000: 1.659906 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "================================================================================\n",
      "ft pneging timaly acting cledderncy peryed bernon coibog as and to fiviman also \n",
      "juar weal totoriciif arguars of the rick that lebwernated causels s lent leal be\n",
      "jain that the preseving the deserved in may severh are expone ciroxn this cal  i\n",
      "ced and strestrataly his anusarticimy name eritics adduch prayary and lively or \n",
      "on orieliom and is the provented by an the legue sunceltev inflectics of epugala\n",
      "================================================================================\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 3100: 1.654233 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 3200: 1.651182 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 3300: 1.636151 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 3400: 1.635122 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 3500: 1.628640 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 3600: 1.627672 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 3700: 1.632428 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 3800: 1.621422 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 3900: 1.616676 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 4000: 1.617203 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "================================================================================\n",
      "ord bakely after the dyan hasbno diving of the trictly timer is karishines ampas\n",
      "u they hiulatoriy a drevidee is allo fording km ic materacultis compleussalyn in\n",
      "cornor to actrons a desibn ussistes and in a jervisuchs technists k d and is sco\n",
      "urership by demodrativity its or also three huila in chimenting adgrised carlysi\n",
      "ney rewallhangs on insulula was defisific a prombas gioroughted for the her ekgu\n",
      "================================================================================\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 4100: 1.617509 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 4200: 1.602209 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 4300: 1.584182 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 4400: 1.618098 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.93\n",
      "Average loss at step 4500: 1.625344 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 4600: 1.624266 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 4700: 1.601149 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 4800: 1.582035 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 4900: 1.592833 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 5000: 1.617651 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.37\n",
      "================================================================================\n",
      "chile war wroth of had be gram do policu intecre pboture and uba axany jemed in \n",
      "vel sp is getlaulds played asiabwab of the use a domi with of steamer as grompte\n",
      "wnotory term in class i plane numment in dhounty janker wift thebran whowe of cl\n",
      "s this pardions ebmossian alperpo the prodyple and in this the breque trans cub \n",
      "tels the experwand on the femaly tyhallarols consirfers s one four two natus sec\n",
      "================================================================================\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 5100: 1.632788 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 5200: 1.620033 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 5300: 1.590978 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 5400: 1.584497 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 5500: 1.577850 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 5600: 1.606307 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 5700: 1.564879 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.03\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 5800: 1.569732 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5900: 1.587432 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 6000: 1.554380 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.20\n",
      "================================================================================\n",
      "ne as corkoast the warl as in the matter the tringa works of as is one beth laik\n",
      "gapleles that chohave to of husto deals bepestwot by discelter playerel eleat in\n",
      "harma becorl and untiviat in interest will direcrage nean are haudhyby pohoyargy\n",
      "in originally area relateds abortal and former and molrews to chacs but three fi\n",
      "ote in their past was impleted and lawing sing one nine nine tereth batters refe\n",
      "================================================================================\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 6100: 1.578025 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 6200: 1.595906 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 6300: 1.605375 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 6400: 1.628475 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 6500: 1.629319 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 6600: 1.604954 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 6700: 1.593462 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 6800: 1.574693 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 6900: 1.567554 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 7000: 1.580356 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "================================================================================\n",
      "x been withomest the rest cresting his freeton miscological of might destriction\n",
      "ame of say anitimenic air varuers tm in the conseutions in the sporezacchation s\n",
      "ver syze vigw non lamace was house the ently in the mount mans the weighm chotwo\n",
      " what the six zero zero four finesse ho re any unenins to be at the trast argue \n",
      " in a dife states parts and extend zero of the militair so notely possion in the\n",
      "================================================================================\n",
      "Validation set perplexity: 4.48\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph_1) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run(\n",
    "            [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchists advocat', 'when military governme', 'lleria arches national', ' abbeys and monasterie', 'married urraca princes', 'hel and richard baer h', 'y and liturgical langu', 'ay opened for passenge', 'tion from the national', 'migration took place d', 'new york other well kn', 'he boeing seven six se', 'e listed with a gloss ', 'eber has probably been', 'o be made to recognize', 'yer who received the f', 'ore significant than i', 'a fierce critic of the', ' two six eight in sign', 'aristotle s uncaused c', 'ity can be lost as in ', ' and intracellular ice', 'tion of the size of th', 'dy to pass him a stick', 'f certain drugs confus', 'at it will take to com', 'e convince the priest ', 'ent told him to name i', 'ampaign and barred att', 'rver side standard for', 'ious texts such as eso', 'o capitalize on the gr', 'a duplicate of the ori', 'gh ann es d hiver one ', 'ine january eight marc', 'ross zero the lead cha', 'cal theories classical', 'ast instance the non g', ' dimensional analysis ', 'most holy mormons beli', 't s support or at leas', 'u is still disagreed u', 'e oscillating system e', 'o eight subtypes based', 'of italy languages the', 's the tower commission', 'klahoma press one nine', 'erprise linux suse lin', 'ws becomes the first d', 'et in a nazi concentra', 'the fabian society neh', 'etchy to relatively st', ' sharman networks shar', 'ised emperor hirohito ', 'ting in political init', 'd neo latin most of th', 'th risky riskerdoo ric', 'encyclopedic overview ', 'fense the air componen', 'duating from acnm accr', 'treet grid centerline ', 'ations more than any o', 'appeal of devotional b', 'si have made such devi']\n",
      "['ate social relations b', 'ments failed to revive', 'al park photographic v', 'ies index sacred desti', 'ess of castile daughte', ' h provided a detailed', 'guage among jews manda', 'gers in december one n', 'al media and from pres', ' during the one nine e', 'known manufacturers of', 'seven a widebody jet w', 's covering some of the', 'en one of the most inf', 'ze single acts of meri', ' first card from the d', ' in jersey and guernse', 'he poverty and social ', 'gns of humanity vol th', ' cause so aquinas come', 'n denaturalization and', 'ce formation solution ', 'the input usually meas', 'ck to pull him out but', 'usion inability to ori', 'omplete an operation c', 't of the mistakes of a', ' it fort des moines th', 'ttempts by his opponen', 'ormats for mailboxes i', 'soteric christianity a', 'growing popularity of ', 'riginal document fax m', 'e nine eight zero one ', 'rch eight listing of a', 'haracter lieutenant sh', 'al mechanics and speci', ' gm comparison maize c', 's fundamental applicat', 'lieve the configuratio', 'ast not parliament s o', ' upon by historians an', ' example rlc circuit f', 'ed on the whole genome', 'he official language o', 'on at this point presi', 'ne three two one one t', 'inux enterprise server', ' daily college newspap', 'ration camp lewis has ', 'ehru wished the econom', 'stiff from flat to tig', 'arman s sydney based b', 'o to begin negotiation', 'itiatives the lesotho ', 'these authors wrote in', 'icky ricardo this clas', 'w of mathematics prese', 'ent of arm is represen', 'credited programs must', 'e external links bbc o', ' other state modern da', ' buddhism especially r', 'vices possible the sys']\n",
      "[' ana']\n",
      "['narc']\n"
     ]
    }
   ],
   "source": [
    "num_unrollings=10\n",
    "num_alpha = len(string.ascii_lowercase) + 1\n",
    "vocabulary_size = num_alpha * num_alpha\n",
    "\n",
    "def bigram2Index(c1, c2):\n",
    "    return  char2id(c1) * num_alpha + char2id(c2)\n",
    "\n",
    "def index2Bigram(idx):\n",
    "    c1 = id2char(idx // num_alpha)\n",
    "    c2 = id2char(idx % num_alpha)\n",
    "    return c1+c2\n",
    "\n",
    "class BigramBatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "    \n",
    "    def _next_txt_pos(self, i):\n",
    "        return (i+1) % self._text_size\n",
    "    \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            pos = self._cursor[b]\n",
    "            c1 = self._text[pos]\n",
    "            pos = self._next_txt_pos(pos)\n",
    "            c2 = self._text[pos]\n",
    "            batch[b, bigram2Index(c1, c2)] = 1.0\n",
    "            self._cursor[b] = self._next_txt_pos(pos)\n",
    "        return batch\n",
    "    \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "def bigram_characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [index2Bigram(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def bigram_batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, bigram_characters(b))]\n",
    "    return s\n",
    "\n",
    "train_batches_2 = BigramBatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches_2 = BigramBatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(bigram_batches2string(train_batches_2.next()))\n",
    "print(bigram_batches2string(train_batches_2.next()))\n",
    "print(bigram_batches2string(valid_batches_2.next()))\n",
    "print(bigram_batches2string(valid_batches_2.next()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bigram-based LSTM\n",
    "# introduce dropout\n",
    "num_nodes = 64\n",
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "dropout_keep_rate = 0.9\n",
    "\n",
    "graph_2 = tf.Graph()\n",
    "with graph_2.as_default():\n",
    "    \n",
    "    # Parameters: input, previous output and bias    \n",
    "    ifcox = tf.Variable(tf.truncated_normal([embedding_size, 4 * num_nodes], -0.1, 0.1)) #represent a bigram using idx(c1) * 27 + idx(c2)\n",
    "    ifcom = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "    ifcob = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "    \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size, embedding_size], -0.1, 0.1))\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        all_gate = tf.matmul(i, ifcox) + tf.matmul(o, ifcom) + ifcob\n",
    "        input_gate = tf.sigmoid(all_gate[:, 0: num_nodes])\n",
    "        forget_gate = tf.sigmoid(all_gate[:, num_nodes: 2* num_nodes])\n",
    "        update = all_gate[:, 2*num_nodes: 3* num_nodes]\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(all_gate[:, 3*num_nodes:])\n",
    "        return output_gate * tf.tanh(state), state\n",
    "    \n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]    # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        #embed = tf.nn.embedding_lookup(embeddings, tf.argmax(i, dimension=1))\n",
    "        embed = tf.matmul(i, embeddings)\n",
    "        output, state = lstm_cell(tf.nn.dropout(embed, dropout_keep_rate), output, state)\n",
    "        outputs.append(tf.nn.dropout(output, dropout_keep_rate))\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    #sample_embed = tf.nn.embedding_lookup(embeddings, tf.argmax(sample_input, dimension=1))\n",
    "    sample_embed = tf.matmul(sample_input, embeddings)\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_embed, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.592343 learning rate: 10.000000\n",
      "Minibatch perplexity: 729.49\n",
      "================================================================================\n",
      "yrfndlaxaihlscfqbmqkvgupkjyo uvllphmlsarljmcjaklbhekchyzpvkhncjzrdxqyifadiqvzvtv\n",
      "wmcwp olsxigjhxbobyemvcbkfhltjavaeeoqndtmmlunequmelj qnigkpzuv orrixez ykidyy yo\n",
      "sbscaxzadivnzmhbm vffuxzorvt arrbplpssqtptkhehhyincfxlykocfpqcpjkz alcsrlhhswgig\n",
      "mxnvowijolulnfyasfbrrtg pvhmnsfsddyglqzggchlll a hefxxfqeuvciwtupjudofelsfihqkop\n",
      "lkkroaxpdkqlqtxdmdrfujsvlue dlrdapcwaurlaqnsuhg rbezbxpwhvjpytkn lkbyzdcnafktjfr\n",
      "================================================================================\n",
      "Validation set perplexity: 674.72\n",
      "Average loss at step 100: 5.319384 learning rate: 10.000000\n",
      "Minibatch perplexity: 124.71\n",
      "Validation set perplexity: 140.27\n",
      "Average loss at step 200: 4.644044 learning rate: 10.000000\n",
      "Minibatch perplexity: 76.54\n",
      "Validation set perplexity: 91.46\n",
      "Average loss at step 300: 4.255024 learning rate: 10.000000\n",
      "Minibatch perplexity: 59.58\n",
      "Validation set perplexity: 74.69\n",
      "Average loss at step 400: 4.006368 learning rate: 10.000000\n",
      "Minibatch perplexity: 62.09\n",
      "Validation set perplexity: 63.95\n",
      "Average loss at step 500: 3.899250 learning rate: 10.000000\n",
      "Minibatch perplexity: 51.69\n",
      "Validation set perplexity: 57.61\n",
      "Average loss at step 600: 3.800279 learning rate: 10.000000\n",
      "Minibatch perplexity: 48.11\n",
      "Validation set perplexity: 48.51\n",
      "Average loss at step 700: 3.797200 learning rate: 10.000000\n",
      "Minibatch perplexity: 39.26\n",
      "Validation set perplexity: 43.60\n",
      "Average loss at step 800: 3.746233 learning rate: 10.000000\n",
      "Minibatch perplexity: 45.33\n",
      "Validation set perplexity: 43.49\n",
      "Average loss at step 900: 3.707549 learning rate: 10.000000\n",
      "Minibatch perplexity: 46.02\n",
      "Validation set perplexity: 39.79\n",
      "Average loss at step 1000: 3.596593 learning rate: 10.000000\n",
      "Minibatch perplexity: 36.25\n",
      "================================================================================\n",
      "kp sited to inpase the unive takes bi world displeador its bhilled locber for fo\n",
      "jmonaln in the surrn amerigia ubstant colodition worth to matherish expented boc\n",
      "pzly game hist to gloce for expite fin hot helarguage that infect of one one nin\n",
      "eoxinefies bas form he woun orullic inx stine of wuam the plcan falty clree of o\n",
      "do bem boucceducic base they nuct to by mone romenon ppectial allist dan phosius\n",
      "================================================================================\n",
      "Validation set perplexity: 40.64\n",
      "Average loss at step 1100: 3.549758 learning rate: 10.000000\n",
      "Minibatch perplexity: 33.83\n",
      "Validation set perplexity: 35.82\n",
      "Average loss at step 1200: 3.601530 learning rate: 10.000000\n",
      "Minibatch perplexity: 35.77\n",
      "Validation set perplexity: 36.86\n",
      "Average loss at step 1300: 3.559452 learning rate: 10.000000\n",
      "Minibatch perplexity: 35.42\n",
      "Validation set perplexity: 34.87\n",
      "Average loss at step 1400: 3.563757 learning rate: 10.000000\n",
      "Minibatch perplexity: 40.55\n",
      "Validation set perplexity: 33.45\n",
      "Average loss at step 1500: 3.530719 learning rate: 10.000000\n",
      "Minibatch perplexity: 33.03\n",
      "Validation set perplexity: 34.75\n",
      "Average loss at step 1600: 3.461440 learning rate: 10.000000\n",
      "Minibatch perplexity: 37.19\n",
      "Validation set perplexity: 32.82\n",
      "Average loss at step 1700: 3.497096 learning rate: 10.000000\n",
      "Minibatch perplexity: 33.53\n",
      "Validation set perplexity: 30.73\n",
      "Average loss at step 1800: 3.471366 learning rate: 10.000000\n",
      "Minibatch perplexity: 34.93\n",
      "Validation set perplexity: 31.56\n",
      "Average loss at step 1900: 3.473411 learning rate: 10.000000\n",
      "Minibatch perplexity: 33.22\n",
      "Validation set perplexity: 29.78\n",
      "Average loss at step 2000: 3.390679 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.30\n",
      "================================================================================\n",
      "fibluse is a state stresout willing ad wacchquel by b there is older trate ren t\n",
      " re bit and riws exantose which and mathary h metearut yorks from later enturaha\n",
      "gvvzate the west eraughargo hai etauce as lowed the jastil presertury stest thre\n",
      "zued male jaxipatized in houlders in this debanity first mations be fromar proco\n",
      "nholle eass of myxthriod accwonation riere at the logy zero k the retronate beha\n",
      "================================================================================\n",
      "Validation set perplexity: 31.35\n",
      "Average loss at step 2100: 3.439705 learning rate: 10.000000\n",
      "Minibatch perplexity: 34.31\n",
      "Validation set perplexity: 29.98\n",
      "Average loss at step 2200: 3.422534 learning rate: 10.000000\n",
      "Minibatch perplexity: 33.42\n",
      "Validation set perplexity: 27.35\n",
      "Average loss at step 2300: 3.448276 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.37\n",
      "Validation set perplexity: 26.98\n",
      "Average loss at step 2400: 3.468612 learning rate: 10.000000\n",
      "Minibatch perplexity: 34.50\n",
      "Validation set perplexity: 26.43\n",
      "Average loss at step 2500: 3.360911 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.96\n",
      "Validation set perplexity: 26.06\n",
      "Average loss at step 2600: 3.382907 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.50\n",
      "Validation set perplexity: 24.97\n",
      "Average loss at step 2700: 3.358600 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.14\n",
      "Validation set perplexity: 26.38\n",
      "Average loss at step 2800: 3.394809 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.73\n",
      "Validation set perplexity: 25.10\n",
      "Average loss at step 2900: 3.406793 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.70\n",
      "Validation set perplexity: 24.79\n",
      "Average loss at step 3000: 3.393358 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.10\n",
      "================================================================================\n",
      "bl land with also s proteodisly of the them decepture come high gess styer of fe\n",
      "watiey depublic on the totally and server compin the docikal dhare impreit nari \n",
      "pmentibin all hound one nine six univer prenteols had hower to beantiones even s\n",
      "yg and he breadism as also can ugos rcoou and stanse gamary in a rett is the irr\n",
      "lqris scanned wronl uch and the dumos trays alti arrenked sprive experent and in\n",
      "================================================================================\n",
      "Validation set perplexity: 24.56\n",
      "Average loss at step 3100: 3.394857 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.07\n",
      "Validation set perplexity: 25.08\n",
      "Average loss at step 3200: 3.360231 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.59\n",
      "Validation set perplexity: 25.28\n",
      "Average loss at step 3300: 3.340980 learning rate: 10.000000\n",
      "Minibatch perplexity: 31.97\n",
      "Validation set perplexity: 23.93\n",
      "Average loss at step 3400: 3.310677 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.47\n",
      "Validation set perplexity: 22.97\n",
      "Average loss at step 3500: 3.275587 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.03\n",
      "Validation set perplexity: 23.35\n",
      "Average loss at step 3600: 3.303206 learning rate: 10.000000\n",
      "Minibatch perplexity: 22.71\n",
      "Validation set perplexity: 23.30\n",
      "Average loss at step 3700: 3.317749 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.20\n",
      "Validation set perplexity: 24.29\n",
      "Average loss at step 3800: 3.337569 learning rate: 10.000000\n",
      "Minibatch perplexity: 33.71\n",
      "Validation set perplexity: 23.81\n",
      "Average loss at step 3900: 3.282229 learning rate: 10.000000\n",
      "Minibatch perplexity: 23.12\n",
      "Validation set perplexity: 23.51\n",
      "Average loss at step 4000: 3.246412 learning rate: 10.000000\n",
      "Minibatch perplexity: 32.52\n",
      "================================================================================\n",
      "lj to v one nine nine zero six four zero zero zero mas produce fire generar of a\n",
      "ahmed of the word saky beforoperan have matinia s capaland iffedson and de new i\n",
      "cf sin common levil publiclations gensinves a consists brough and moved of bogke\n",
      "her to parbdative interming duts major aroys albon numen and is the grartime rob\n",
      "gwolder conasa projection colors controlizentation k jusue of the eurougond g of\n",
      "================================================================================\n",
      "Validation set perplexity: 24.26\n",
      "Average loss at step 4100: 3.315501 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.39\n",
      "Validation set perplexity: 23.17\n",
      "Average loss at step 4200: 3.288049 learning rate: 10.000000\n",
      "Minibatch perplexity: 32.74\n",
      "Validation set perplexity: 24.06\n",
      "Average loss at step 4300: 3.249754 learning rate: 10.000000\n",
      "Minibatch perplexity: 23.78\n",
      "Validation set perplexity: 23.29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 4400: 3.264284 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.49\n",
      "Validation set perplexity: 22.96\n",
      "Average loss at step 4500: 3.284672 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.54\n",
      "Validation set perplexity: 22.96\n",
      "Average loss at step 4600: 3.276545 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.23\n",
      "Validation set perplexity: 22.35\n",
      "Average loss at step 4700: 3.299826 learning rate: 10.000000\n",
      "Minibatch perplexity: 22.44\n",
      "Validation set perplexity: 23.28\n",
      "Average loss at step 4800: 3.265827 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.56\n",
      "Validation set perplexity: 22.72\n",
      "Average loss at step 4900: 3.249930 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.77\n",
      "Validation set perplexity: 24.05\n",
      "Average loss at step 5000: 3.288751 learning rate: 1.000000\n",
      "Minibatch perplexity: 32.78\n",
      "================================================================================\n",
      "vk boom jown have struw elacement however fact brdper bombers more also the jewi\n",
      "wd a morfistal frek in the director techorited and ang this of stille to it is h\n",
      "ture futlynnumaused as the paroup to be were nike a part have ths work primary a\n",
      "lxia pee new german scrone in conception cloptian new pracattriansy fullian the \n",
      "luoh one eight three zero zero two five two stain greaving int only mably b one \n",
      "================================================================================\n",
      "Validation set perplexity: 22.76\n",
      "Average loss at step 5100: 3.249773 learning rate: 1.000000\n",
      "Minibatch perplexity: 20.20\n",
      "Validation set perplexity: 22.02\n",
      "Average loss at step 5200: 3.275494 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.96\n",
      "Validation set perplexity: 21.62\n",
      "Average loss at step 5300: 3.303429 learning rate: 1.000000\n",
      "Minibatch perplexity: 31.95\n",
      "Validation set perplexity: 21.35\n",
      "Average loss at step 5400: 3.280948 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.69\n",
      "Validation set perplexity: 21.23\n",
      "Average loss at step 5500: 3.303564 learning rate: 1.000000\n",
      "Minibatch perplexity: 30.17\n",
      "Validation set perplexity: 21.11\n",
      "Average loss at step 5600: 3.358755 learning rate: 1.000000\n",
      "Minibatch perplexity: 34.36\n",
      "Validation set perplexity: 20.75\n",
      "Average loss at step 5700: 3.325431 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.73\n",
      "Validation set perplexity: 20.61\n",
      "Average loss at step 5800: 3.249770 learning rate: 1.000000\n",
      "Minibatch perplexity: 20.41\n",
      "Validation set perplexity: 20.35\n",
      "Average loss at step 5900: 3.287857 learning rate: 1.000000\n",
      "Minibatch perplexity: 28.34\n",
      "Validation set perplexity: 20.67\n",
      "Average loss at step 6000: 3.316663 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.06\n",
      "================================================================================\n",
      "kle taggy vitaer adminimen wall ways fmerbion and times oflifortagean zero eight\n",
      "ydn after sha river between their sady p non delawkked to upars doess allecrosfr\n",
      "mzba language fend which the calred of internate for selain precises that for fa\n",
      "ciie sourch aleasm from the nine midge large wed a last of josplered to the b on\n",
      " is culg from fojtder one nine four two zero fut endice apprusters to golf pinti\n",
      "================================================================================\n",
      "Validation set perplexity: 20.82\n",
      "Average loss at step 6100: 3.316485 learning rate: 1.000000\n",
      "Minibatch perplexity: 29.86\n",
      "Validation set perplexity: 20.79\n",
      "Average loss at step 6200: 3.288253 learning rate: 1.000000\n",
      "Minibatch perplexity: 27.22\n",
      "Validation set perplexity: 20.75\n",
      "Average loss at step 6300: 3.267042 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.36\n",
      "Validation set perplexity: 20.60\n",
      "Average loss at step 6400: 3.266105 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.48\n",
      "Validation set perplexity: 20.21\n",
      "Average loss at step 6500: 3.285231 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.85\n",
      "Validation set perplexity: 20.28\n",
      "Average loss at step 6600: 3.300348 learning rate: 1.000000\n",
      "Minibatch perplexity: 28.62\n",
      "Validation set perplexity: 20.41\n",
      "Average loss at step 6700: 3.287974 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.57\n",
      "Validation set perplexity: 20.57\n",
      "Average loss at step 6800: 3.301956 learning rate: 1.000000\n",
      "Minibatch perplexity: 27.34\n",
      "Validation set perplexity: 20.21\n",
      "Average loss at step 6900: 3.258046 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.70\n",
      "Validation set perplexity: 20.17\n",
      "Average loss at step 7000: 3.225581 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.55\n",
      "================================================================================\n",
      "rj camment only infront ofing an were rait history spabled had mafical well is a\n",
      "iked s one nine seven eight six massrk in feeting been placess hades been as in \n",
      "qb there as to be force novel can with modern god will was which consection and \n",
      "izt shrertakesway were nind two zero zero by effected some of mallanges in one f\n",
      "zjs exilestian am importrable as an orbiphery these of vistural as a word one ei\n",
      "================================================================================\n",
      "Validation set perplexity: 19.97\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph_2) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches_2.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run(\n",
    "            [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = bigram_characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(39):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += bigram_characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches_2.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code, I will make use of the 2 classes from the Google English to French translator, they are seq2seq_model.py and data_utils.py\n",
    "\n",
    "\n",
    "Please download the code from https://github.com/tensorflow/models/tree/master/tutorials/rnn/translate to local directory to run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import seq2seq_model as seq2seq_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set\n",
      "['PPPsuiluj fo eidegart eht', 'PPPPPPPPPPPPPsumirp sutca', 'PPPPPPPPPPPPPamirp aneocs', 'PPPsullerum suiualf retne', 'PPPPPPPPPPPPPPPPPPsuiualf', 'PPPPPPeldi uoy emoh ecneh', 'PPPPPPPPton uoy wonk tahw', 'PPPPtra edart tahw ekaeps', 'PPPPPPPPPPPPPPPPPPPPPPrac', 'PPPPPPretneprac a ris yhw', 'PPPPPPPPPPPPPPPPPPPPPPrum', 'PPPPPrehtael yht si erehw', 'PPPPPPhtiw uoht tsod tahw', 'PPPera edart tahw ris uoy', 'PPPPPPPPPPPPPPPPPPPPPlboc', 'PPPPtcepser ni ris yleurt', 'PPPPPPPPPPPPPPPPPPPPPPrum', 'PPPPPPPtra edart tahw tub', 'PPPPPPPyltcerid em rewsna', 'PPPPPPPPPPPPPPPPPPPPPPboc', 'PPPPPPPi taht ris edart a', 'PPPPPPuoht edart tahw alf', 'PPPPPPPeuank ythguan uoht', 'PPPPPPPPPPPPPPPPPPPPPlboc', 'PPPPris uoy hceeseb i yan', 'PPPPPPPPPPPPPPPPPPPPPPrum', 'PPPPPyb uoht ts naem tahw', 'PPPPPPycwas uoht eem dnem', 'PPPPPPPPPPPPPPPPPPPPPPboc', 'PPPPPPPuoy elbboc ris yhw', 'PPPPrelboc a tra uoht alf', 'PPPPPPPPPPPPPPPPPPPPPPboc']\n",
      "['Geht eidegart fo suilujEP', 'Gsutca sumirpEPPPPPPPPPPP', 'Ganeocs amirpEPPPPPPPPPPP', 'Gretne suiualf sullerumEP', 'GsuiualfEPPPPPPPPPPPPPPPP', 'Gecneh emoh uoy eldiEPPPP', 'Gtahw wonk uoy tonEPPPPPP', 'Gekaeps tahw edart traEPP', 'GracEPPPPPPPPPPPPPPPPPPPP', 'Gyhw ris a retnepracEPPPP', 'GrumEPPPPPPPPPPPPPPPPPPPP', 'Gerehw si yht rehtaelEPPP', 'Gtahw tsod uoht htiwEPPPP', 'Guoy ris tahw edart eraEP', 'GlbocEPPPPPPPPPPPPPPPPPPP', 'Gyleurt ris ni tcepserEPP', 'GrumEPPPPPPPPPPPPPPPPPPPP', 'Gtub tahw edart traEPPPPP', 'Grewsna em yltceridEPPPPP', 'GbocEPPPPPPPPPPPPPPPPPPPP', 'Ga edart ris taht iEPPPPP', 'Galf tahw edart uohtEPPPP', 'Guoht ythguan euankEPPPPP', 'GlbocEPPPPPPPPPPPPPPPPPPP', 'Gyan i hceeseb uoy risEPP', 'GrumEPPPPPPPPPPPPPPPPPPPP', 'Gtahw naem ts uoht ybEPPP', 'Gdnem eem uoht ycwasEPPPP', 'GbocEPPPPPPPPPPPPPPPPPPPP', 'Gyhw ris elbboc uoyEPPPPP', 'Galf uoht tra a relbocEPP', 'GbocEPPPPPPPPPPPPPPPPPPPP']\n",
      "Validation set\n",
      "['PPPPPPxof nworb kciuq eht', 'PPPPsub yb loohcs ot og i']\n",
      "['Geht kciuq nworb xofEPPPP', 'Gi og ot loohcs yb subEPP']\n"
     ]
    }
   ],
   "source": [
    "# Model parameters\n",
    "learning_rate = 0.5\n",
    "learning_rate_decay = 0.9\n",
    "max_gradient_norm = 5\n",
    "layer_size = 64     #Layer size\n",
    "num_layers = 3      #Number of layer \n",
    "vocabulary_size = len(string.ascii_lowercase) + 4 # a-z, space, GO, PAD, EOS\n",
    "bucket_size = 25\n",
    "buckets = [(bucket_size,bucket_size)]  # define the encoder and decoder length\n",
    "batch_size = 32\n",
    "\n",
    "class InvertBatchGenerator(object):\n",
    "    _EOS = 28\n",
    "    _PAD = 27\n",
    "    _GO = 29\n",
    "    \n",
    "    def __init__(self, sentences, batch_size, bucket_size):\n",
    "        self._sentences = sentences\n",
    "        self._sentence_size = len(sentences)\n",
    "        self._batch_size = batch_size\n",
    "        self._bucket_size = bucket_size \n",
    "        self._cursor = 0\n",
    "    \n",
    "    def _create_encoder_decoder_input(self, sentence, max_size):\n",
    "        sent = self._sentences[self._cursor]\n",
    "        sentence = \"\"\n",
    "        sentence_r = \"\"\n",
    "        for word in sent:\n",
    "            s_word = re.sub(\"[^a-z ]+\", \"\", word.lower()).strip()\n",
    "            if(len(s_word) == 0):\n",
    "                continue\n",
    "            if(len(sentence) + len(s_word) + 1 > max_size):\n",
    "                break\n",
    "            sentence = sentence + ' ' + s_word.lower()\n",
    "            sentence_r = sentence_r + ' ' + s_word.lower()[::-1]\n",
    "        \n",
    "        return sentence.strip(), sentence_r.strip()\n",
    "        \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Return the following info in one-hot-encoding form\n",
    "            the quick brown fox, eht kciuq nworb xof\n",
    "        \"\"\"\n",
    "        sent, r_sent = self._create_encoder_decoder_input(self._sentences[self._cursor], self._bucket_size-2)\n",
    "        self._cursor = (self._cursor + 1) % self._sentence_size\n",
    "        \n",
    "        padding_len = self._bucket_size - len(sent)\n",
    "        pad_sent = []\n",
    "        pad_r_sent = []\n",
    "        pad_sent.extend(reversed([char2id(c) for c in sent] + ([self._PAD] *  padding_len)))\n",
    "        pad_r_sent.extend([self._GO])\n",
    "        pad_r_sent.extend([char2id(c) for c in r_sent])\n",
    "        pad_r_sent.extend([self._EOS])\n",
    "        pad_r_sent.extend([self._PAD] * (padding_len-2))\n",
    "        \n",
    "        return pad_sent, pad_r_sent\n",
    "    \n",
    "    def next(self):\n",
    "        \"\"\"Generate three output\n",
    "            Encoder input: batch major, size = batch_size * encoder_input_size (bucket_size)\n",
    "            Decoder input: batch major, size = batch_size * decoder_input_size (bucket_size)\n",
    "            Target weight: batch major, size = batch_size * decoder_input_size (bucket_size)\n",
    "        \"\"\"\n",
    "        encoder_inputs = []\n",
    "        decoder_inputs = []\n",
    "        for step in range(self._batch_size):\n",
    "            encoder_input, decoder_input = self._next_batch()\n",
    "            encoder_inputs.append(encoder_input)\n",
    "            decoder_inputs.append(decoder_input)\n",
    "            \n",
    "        batch_encoder_inputs, batch_decoder_inputs, batch_weights = [], [], []\n",
    "        for length_idx in range(self._bucket_size):\n",
    "            batch_encoder_inputs.append(\n",
    "                np.array([encoder_inputs[batch_idx][length_idx] \n",
    "                          for batch_idx in range(self._batch_size)], dtype=np.int32))\n",
    "            \n",
    "        for length_idx in range(self._bucket_size):\n",
    "            batch_decoder_inputs.append(\n",
    "                np.array([decoder_inputs[batch_idx][length_idx] \n",
    "                          for batch_idx in range(self._batch_size)], dtype=np.int32))\n",
    "        \n",
    "            batch_weight = np.ones(self._batch_size, dtype=np.float32)\n",
    "            for batch_idx in range(self._batch_size):\n",
    "                if length_idx < self._bucket_size -1:\n",
    "                    target = decoder_inputs[batch_idx][length_idx + 1]\n",
    "                if length_idx == self._bucket_size - 1 or target == self._PAD:\n",
    "                    batch_weight[batch_idx] = 0.0\n",
    "            batch_weights.append(batch_weight)\n",
    "            \n",
    "        return batch_encoder_inputs, batch_decoder_inputs, batch_weights\n",
    "\n",
    "def id2char_inv(dictid):\n",
    "    if dictid > 0 and dictid <= 26:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    elif dictid == 0:\n",
    "        return ' '\n",
    "    elif dictid == InvertBatchGenerator._EOS:\n",
    "        return 'E'\n",
    "    elif dictid == InvertBatchGenerator._GO:\n",
    "        return 'G'\n",
    "    elif dictid == InvertBatchGenerator._PAD:\n",
    "        return 'P'\n",
    "    else:\n",
    "        return ' '\n",
    "    \n",
    "def characters_inv(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    #return [id2char_inv(c) for c in np.argmax(probabilities)]\n",
    "    return id2char_inv(np.argmax(probabilities))\n",
    "\n",
    "def batches2string_inv(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, [id2char_inv(c) for c in b])]\n",
    "        \n",
    "    return s\n",
    "\n",
    "#def batches2string(batches):\n",
    "#    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "#    representation.\"\"\"\n",
    "#    s = [''] * batches[0].shape[0]\n",
    "#    for b in batches:\n",
    "#        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "#    return s\n",
    "\n",
    "valid_sentences = [[\"the\", \"quick\", \"brown\", \"fox\"], \n",
    "                   [\"I\", \"go\", \"to\", \"school\", \"by\", \"bus\"]]\n",
    "train_sentences = nltk.corpus.gutenberg.sents('shakespeare-caesar.txt')\n",
    "\n",
    "train_batches_inv = InvertBatchGenerator(train_sentences, batch_size, bucket_size)\n",
    "valid_batches_inv = InvertBatchGenerator(valid_sentences, batch_size, bucket_size)\n",
    "\n",
    "print(\"Training set\")\n",
    "encoder_i, decoder_i, weight = train_batches_inv.next()\n",
    "print(batches2string_inv(encoder_i))\n",
    "print(batches2string_inv(decoder_i))\n",
    "\n",
    "print(\"Validation set\")\n",
    "encoder_i, decoder_i, weight = valid_batches_inv.next()\n",
    "print(batches2string_inv(encoder_i)[0:2])\n",
    "print(batches2string_inv(decoder_i)[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "     return seq2seq_model.Seq2SeqModel(source_vocab_size=vocabulary_size,\n",
    "                                   target_vocab_size=vocabulary_size,\n",
    "                                   buckets=buckets, # only 1 bucket\n",
    "                                   size=layer_size,\n",
    "                                   num_layers=num_layers,\n",
    "                                   max_gradient_norm=max_gradient_norm,\n",
    "                                   batch_size=batch_size,\n",
    "                                   learning_rate=learning_rate,\n",
    "                                   learning_rate_decay_factor=learning_rate_decay,\n",
    "                                   use_lstm=True,\n",
    "                                   forward_only=False)\n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "WARNING:tensorflow:From <ipython-input-12-cbfb5b976416>:20: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Average loss at step 0: 3.412030\n",
      "=========================\t=========================\n",
      "eeeEEEEEEEEEEEEEEEEEEEEEE\tGainruhplacEPPPPPPPPPPPPP\n",
      "eeeEEEEEEEEEEEEEEEEEEEEEE\tGksacEPPPPPPPPPPPPPPPPPPP\n",
      " EEEEEEEEEEEEEEEEEEEEEEEE\tGecaep oh raseacEPPPPPPPP\n",
      "eeEEEEEEEEEEEEEEEEEEEEEEE\tGseacEPPPPPPPPPPPPPPPPPPP\n",
      "eeeEEEEEEEEEEEEEEEEEEEEEE\tGainruhplacEPPPPPPPPPPPPP\n",
      "eeEEEEEEEEEEEEEEEEEEEEEEE\tGplacEPPPPPPPPPPPPPPPPPPP\n",
      " EEEEEEEEEEEEEEEEEEEEEEEE\tGereeh ym drolEPPPPPPPPPP\n",
      "eeEEEEEEEEEEEEEEEEEEEEEEE\tGseacEPPPPPPPPPPPPPPPPPPP\n",
      "  EEEEEEEEEEEE           \tGdnats uoy yltcerid niEPP\n",
      " EEEEEEEEEEEEEEEEEEEEEEEE\tGoinotnaEPPPPPPPPPPPPPPPP\n",
      "eeEEEEEEEEEEEEEEEEEEEEEEE\tGtnaEPPPPPPPPPPPPPPPPPPPP\n",
      "  EEEEEEEEEEEEEEEEEEEEEEE\tGrasc ym drolEPPPPPPPPPPP\n",
      "eeEEEEEEEEEEEEEEEEEEEEEEE\tGseacEPPPPPPPPPPPPPPPPPPP\n",
      "EEEEEEEEEEEEEEEEEEEEEEEEE\tGtegrof ton ni ruoyEPPPPP\n",
      "eeEEEEEEEEEEEEEEEEEEEEEEE\tGtnaEPPPPPPPPPPPPPPPPPPPP\n",
      "     EEEEE               \tGi llahs rebmemer nehwEPP\n",
      "eeEEEEEEEEEEEEEEEEEEEEEEE\tGseacEPPPPPPPPPPPPPPPPPPP\n",
      " EEEEEEEEEEEEEEEEEE      \tGtes no dna euael onEPPPP\n",
      " eEEEEEEEEEEEEEEEEEEEEEEE\tGhtoosEPPPPPPPPPPPPPPPPPP\n",
      " eEEEEEEEEEEEEEEEEEEEEEEE\tGraseacEPPPPPPPPPPPPPPPPP\n",
      "eeEEEEEEEEEEEEEEEEEEEEEEE\tGseacEPPPPPPPPPPPPPPPPPPP\n",
      "eeEEEEEEEEEEEEEEEEEEEEEEE\tGahEPPPPPPPPPPPPPPPPPPPPP\n",
      "EEEEEEEEEEEEEEEEEEEEEEEEE\tGohw sellacEPPPPPPPPPPPPP\n",
      "eeeEEEEEEEEEEEEEEEEEEEEEE\tGksacEPPPPPPPPPPPPPPPPPPP\n",
      "EEEEEEEEEEEEEEEEEEEEEEEEE\tGdib yreue esyon ebEPPPPP\n",
      "=========================\t=========================\n",
      "Average loss at step 3000: 1.239362\n",
      "=========================\t=========================\n",
      "ooo                 nnkpp\tGod os dna tel on namEPPP\n",
      "reeeeebbnnuuuuuuuuuuEEEEE\tGretne suinobertEPPPPPPPP\n",
      "issiiiEEEEEEEEEEEEEEEEEEE\tGissacEPPPPPPPPPPPPPPPPPP\n",
      "eeeeee      nnnnnnnnnnnnn\tGerehw si ynotnaEPPPPPPPP\n",
      "beebbEEEEEEEEEEEEEEEEEEEE\tGbertEPPPPPPPPPPPPPPPPPPP\n",
      "dllo                  aaa\tGdelf ot sih esuoh zamaEP\n",
      "urbbEEEEEEEEEEEEEEEEEEEEE\tGurbEPPPPPPPPPPPPPPPPPPPP\n",
      "sesese             kkEEEE\tGsetaf ew lliw wonkEPPPPP\n",
      "ksaaaEEEEEEEEEEEEEEEEEEEE\tGksacEPPPPPPPPPPPPPPPPPPP\n",
      "yhh                    EE\tGyhw eh taht stuc ffoEPPP\n",
      "urbbEEEEEEEEEEEEEEEEEEEEE\tGurbEPPPPPPPPPPPPPPPPPPPP\n",
      "tnnattttttttt   n       s\tGtnarg taht dna neht siEP\n",
      "eeooooooooooonoo   uuuuuE\tGepoots snamor epootsEPPP\n",
      "issiiiEEEEEEEEEEEEEEEEEEE\tGissacEPPPPPPPPPPPPPPPPPP\n",
      "woooooonnnnn       ssssww\tGpoots neht dna hsawEPPPP\n",
      "urbbEEEEEEEEEEEEEEEEEEEEE\tGurbEPPPPPPPPPPPPPPPPPPPP\n",
      "woob yynn         lllllll\tGwoh ynam semit llahsEPPP\n",
      "issiiiEEEEEEEEEEEEEEEEEEE\tGissacEPPPPPPPPPPPPPPPPPP\n",
      "ooo              llllllll\tGos tfo sa taht llahsEPPP\n",
      "cccc   cc  llll eeeEEEEEE\tGced tahw llahs ewEPPPPPP\n",
      "issiiiEEEEEEEEEEEEEEEEEEE\tGissacEPPPPPPPPPPPPPPPPPP\n",
      "i     uuu       aaaaaaaaa\tGi yreue nam yawaEPPPPPPP\n",
      "suuuuuu        luuuul  dd\tGsuturb llahs edael dnaEP\n",
      "reeeebbb    ppppppEEEEEEE\tGretne a tnauresEPPPPPPPP\n",
      "urbbEEEEEEEEEEEEEEEEEEEEE\tGurbEPPPPPPPPPPPPPPPPPPPP\n",
      "=========================\t=========================\n",
      "Average loss at step 6000: 0.082143\n",
      "=========================\t=========================\n",
      "eeeeeeeuuiiiiiiiiiiuuuull\tGepeels eniaga suiculEPPP\n",
      "raaaEEEEEEEEEEEEEEEEEEEEE\tGravEPPPPPPPPPPPPPPPPPPPP\n",
      "ym  uubEEEEEEEEEEEEEEEEEE\tGym drolEPPPPPPPPPPPPPPPP\n",
      "uuulllllllllllEEEEEEEEEEE\tGualcEPPPPPPPPPPPPPPPPPPP\n",
      "ym  uubEEEEEEEEEEEEEEEEEE\tGym drolEPPPPPPPPPPPPPPPP\n",
      "urbbEEEEEEEEEEEEEEEEEEEEE\tGurbEPPPPPPPPPPPPPPPPPPPP\n",
      "yhw                   uuu\tGyhw did uoy os yrc tuoEP\n",
      "hhbbbbbEEEEEEEEEEEEEEEEEE\tGhtobEPPPPPPPPPPPPPPPPPPP\n",
      "dii          uoEEEEEEEEEE\tGdid ew ym drolEPPPPPPPPP\n",
      "urbbEEEEEEEEEEEEEEEEEEEEE\tGurbEPPPPPPPPPPPPPPPPPPPP\n",
      "i                 gnnnEEE\tGi was uoy yna gnihtEPPPP\n",
      "raaaEEEEEEEEEEEEEEEEEEEEE\tGravEPPPPPPPPPPPPPPPPPPPP\n",
      "oob               EEEEEEE\tGon ym drol i wasEPPPPPPP\n",
      "uuulllllllllllEEEEEEEEEEE\tGualcEPPPPPPPPPPPPPPPPPPP\n",
      "rooob       EEEEEEEEEEEEE\tGron i ym drolEPPPPPPPPPP\n",
      "urbbEEEEEEEEEEEEEEEEEEEEE\tGurbEPPPPPPPPPPPPPPPPPPPP\n",
      "oo       ddd         EEEE\tGog dna dnemmoc em otEPPP\n",
      "hhbbbbbEEEEEEEEEEEEEEEEEE\tGhtobEPPPPPPPPPPPPPPPPPPP\n",
      "tt                  EEEEE\tGti llahs eb enod ymEPPPP\n",
      "tntteeeeeEEEEEEEEEEEEEEEE\tGtnuexeEPPPPPPPPPPPPPPPPP\n",
      "suutuuuuuuuuuuuuEEEuuuuuu\tGsutca sutniuqEPPPPPPPPPP\n",
      "reeeuuuuuuuuuuuuuuuuuuuuu\tGretne suiuatco ynotnaEPP\n",
      "atcccEEEEEEllEEEEEEEEEEEE\tGatcoEPPPPPPPPPPPPPPPPPPP\n",
      "woonnnnnnnnnb        EEEE\tGwon ynotna ruo sepohEPPP\n",
      "tnacEEEEEEEEEEEEEEEEEEEEE\tGtnaEPPPPPPPPPPPPPPPPPPPP\n",
      "=========================\t=========================\n",
      "Average loss at step 9000: 0.001444\n",
      "=========================\t=========================\n",
      "ooo                  uuuu\tGon ris rieht stah eraEPP\n",
      "turbbbEEEEEEEEEEEEEEEEEEE\tGturbEPPPPPPPPPPPPPPPPPPP\n",
      "tee                 uuuuu\tGtel me retne yeht eraEPP\n",
      "ooooiiiiiiiiiiiiiaaaaaaaa\tGo eicaripsnoc mahs tsEPP\n",
      "o                 uuuuEEE\tGo neht yb yad erehwEPPPP\n",
      "keeeee     uuiiiiiiiiiiii\tGkees enon eicaripsnocEPP\n",
      "rreeeee        ooooraaaaa\tGretne eht srotaripsnocEP\n",
      "sssaalEEEEEEEEEEEEEEEEEEs\tGssacEPPPPPPPPPPPPPPPPPPP\n",
      "i   kknnnnn   uuuuuuEEEEE\tGi ekniht ew era ootEPPPP\n",
      "turbbbEEEEEEEEEEEEEEEEEEE\tGturbEPPPPPPPPPPPPPPPPPPP\n",
      "i    uuuuuu         sssEE\tGi euah eneeb pv sihtEPPP\n",
      "sssaalEEEEEEEEEEEEEEEEEEs\tGssacEPPPPPPPPPPPPPPPPPPP\n",
      "sssuu                  EE\tGsey yreue nam fo mehtEPP\n",
      "siiii     uuuuuuuuuEEEEEE\tGsiht si suinobertEPPPPPP\n",
      "turbbbEEEEEEEEEEEEEEEEEEE\tGturbEPPPPPPPPPPPPPPPPPPP\n",
      "ee              hhhhhhhho\tGeh si emoclew rehtihEPPP\n",
      "sssaalEEEEEEEEEEEEEEEEEEs\tGssacEPPPPPPPPPPPPPPPPPPP\n",
      "siiiiiiuuuuuuuuuuuuuuuuuu\tGsiht suiced suturbEPPPPP\n",
      "turbbbEEEEEEEEEEEEEEEEEEE\tGturbEPPPPPPPPPPPPPPPPPPP\n",
      "ee            uuuoEEEEEEE\tGeh si emoclew ootEPPPPPP\n",
      "sssaalEEEEEEEEEEEEEEEEEEs\tGssacEPPPPPPPPPPPPPPPPPPP\n",
      "siiaaa   aaaiiiiiiiiccccc\tGsiht aksac siht annicEPP\n",
      "turbbbEEEEEEEEEEEEEEEEEEE\tGturbEPPPPPPPPPPPPPPPPPPP\n",
      "yeeeee           uullllll\tGyeht era lla emoclewEPPP\n",
      "taahaccccllllluuuuuulllll\tGtahw llufhctaw seracEPPP\n",
      "=========================\t=========================\n",
      "Average loss at step 12000: 0.000468\n",
      "=========================\t=========================\n",
      "tnacEEEEEEEEEEEEEEEEEEEEE\tGtnaEPPPPPPPPPPPPPPPPPPPP\n",
      "ee            uuuuuEEEEEE\tGeh llahs ton euilEPPPPPP\n",
      "ttu   uuuuu          ooEE\tGtub sudipel og uoy otEPP\n",
      "pellEEEEEEEEEEEEEEEEEEEEE\tGpelEPPPPPPPPPPPPPPPPPPPP\n",
      "tahaaEEEEEEEEEEEEEEEEEEEE\tGtahwEPPPPPPPPPPPPPPPPPPP\n",
      "llllll       uuuuuuEEEEEE\tGllahs i ednif uoyEPPPPPP\n",
      "atcccEEEEElllllllEEEEEEEE\tGatcoEPPPPPPPPPPPPPPPPPPP\n",
      "rrrbeeee            EEEEE\tGro ereeh ro ta ehtEPPPPP\n",
      "ttt  uuuuuuuuEEEEEEEEEEEE\tGtixe sudipelEPPPPPPPPPPP\n",
      "tnacEEEEEEEEEEEEEEEEEEEEE\tGtnaEPPPPPPPPPPPPPPPPPPPP\n",
      "siiiii       wwEEEEEEEEEE\tGsiht si a thgilsEPPPPPPP\n",
      "atcccEEEEElllllllEEEEEEEE\tGatcoEPPPPPPPPPPPPPPPPPPP\n",
      "ooo                    EE\tGos uoy thguoht mih dnaEP\n",
      "tnacEEEEEEEEEEEEEEEEEEEEE\tGtnaEPPPPPPPPPPPPPPPPPPPP\n",
      "suuuuuuuuuuuuuuuuu eeeeee\tGsuiuatco i euah eneesEPP\n",
      "atcccEEEEElllllllEEEEEEEE\tGatcoEPPPPPPPPPPPPPPPPPPP\n",
      "uuubb            llllllll\tGuoy yam od ruoy lliwEPPP\n",
      "tnacEEEEEEEEEEEEEEEEEEEEE\tGtnaEPPPPPPPPPPPPPPPPPPPP\n",
      "ooo       uuuueEEEEEEEEEE\tGos si ym esrohEPPPPPPPPP\n",
      "ttt          uu      EEEE\tGti si a erutaerc tahtEPP\n",
      "haaacc             dEEEEE\tGhcihw tuo fo esv dnaEPPP\n",
      "oo                 EEEEEE\tGod ton eklat fo mihEPPPP\n",
      "suuuuuu     iiiiiii  uuuE\tGsuturb dna suissac eraEP\n",
      "atcccEEEEElllllllEEEEEEEE\tGatcoEPPPPPPPPPPPPPPPPPPP\n",
      "tuu                  EEEE\tGtel sv od os rof ewEPPPP\n",
      "=========================\t=========================\n",
      "Average loss at step 15000: 0.000266\n",
      "=========================\t=========================\n",
      "yluuuu             uuuuuu\tGylurt ris ot eraew tuoEP\n",
      "tuu eeeeee         EEEEEE\tGtub edeedni ris ewEPPPPP\n",
      "rrbbEEEEEEEEEEEEEEEEEEEEE\tGrumEPPPPPPPPPPPPPPPPPPPP\n",
      "eeeeeeeeeuuuuuuuuueeeeeee\tGeroferehw ecyoierEPPPPPP\n",
      "tahhtttttttuuuuunnnnnnEEE\tGtahw tseuqnoc sgnirbEPPP\n",
      "taahhtt uuuuuuuuuuuuuuuuu\tGtahw seiratubirtEPPPPPPP\n",
      "uuouuuuuuuuuuuuuuuuuuuuus\tGuoy sekcolb uoy senotsEP\n",
      "euuuuu                 EE\tGeuah uoy bmilc d pv otEP\n",
      "aaoo      uooonnvssssssss\tGaey ot yenmihc spotEPPPP\n",
      "dndc                  EEE\tGdna od uoy won tup noEPP\n",
      "dndc               llllll\tGdna od uoy won llucEPPPP\n",
      "dndc                 EEEE\tGdna od uoy won wertsEPPP\n",
      "eeee    uuunnnuu    uoooE\tGeb enog ennur ot ruoyEPP\n",
      "aaaoo         oEEEEEEEEEE\tGalf og og doogEPPPPPPPPP\n",
      "tntteeeeeEEEEEEEEEEEEEEEE\tGtnuexeEPPPPPPPPPPPPPPPPP\n",
      "llll        uooooooEEEEEE\tGlla eht srenommocEPPPPPP\n",
      "eeeeeeee        sssssssss\tGees erehw rieht tsesabEP\n",
      "rrbbEEEEEEEEEEEEEEEEEEEEE\tGrumEPPPPPPPPPPPPPPPPPPPP\n",
      "yaam         EEEEEEEEEEEE\tGyam ew od osEPPPPPPPPPPP\n",
      "uuuo              EEEEEEE\tGuoy wonk ti si ehtEPPPPP\n",
      "aaalllEEEEEEEEEEEEEEEEEEE\tGalfEPPPPPPPPPPPPPPPPPPPP\n",
      "tt                      E\tGti si on rettam tel onEP\n",
      "eeeee sggggginnnnaaroooEE\tGeseht gniworg srehtaefEP\n",
      "tntteeeeeEEEEEEEEEEEEEEEE\tGtnuexeEPPPPPPPPPPPPPPPPP\n",
      "reeeeerrraaaaaaannnnnnnnn\tGretne raseac ynotnaEPPPP\n",
      "=========================\t=========================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 18000: 0.000183\n",
      "=========================\t=========================\n",
      "kssaallllllEEEEEEEEEEEEEE\tGksacEPPPPPPPPPPPPPPPPPPP\n",
      "eeelllllssss i     EEEEEE\tGekaeps sdnah rof emEPPPP\n",
      "yeeetbbtccaaaaaaaaaaaaaaa\tGyeht bats raseacEPPPPPPP\n",
      "ssaiis     uuuuuuuuuuuuuu\tGseac te ut eturbEPPPPPPP\n",
      "neet     laaaaaaaaaaaaaaa\tGneht llaf raseacEPPPPPPP\n",
      "ssessEEEEEEEEEEEEEEEEEEEE\tGseydEPPPPPPPPPPPPPPPPPPP\n",
      "nncccEEEEEEEaaaaaaaaaaaaa\tGnicEPPPPPPPPPPPPPPPPPPPP\n",
      "yrrrrrruueeeeeeeeeeeeeuuu\tGytrebil emodeerfEPPPPPPP\n",
      "isssiillEEEEEEEEEEEEEEEEE\tGissacEPPPPPPPPPPPPPPPPPP\n",
      "eeooo           ooooooooo\tGemos ot eht nommocEPPPPP\n",
      "urbbEEEEEEEEEEEEEEEEEEEEE\tGurbEPPPPPPPPPPPPPPPPPPPP\n",
      "eeeluuu      ccuoooooo EE\tGelpoep dna srotanes ebEP\n",
      "kssaallllllEEEEEEEEEEEEEE\tGksacEPPPPPPPPPPPPPPPPPPP\n",
      "oo               EEEEEEEE\tGog ot eht tiplupEPPPPPPP\n",
      "cccd    iiiiiiisiluuuEEEE\tGced dna suissac ootEPPPP\n",
      "urbbEEEEEEEEEEEEEEEEEEEEE\tGurbEPPPPPPPPPPPPPPPPPPPP\n",
      "eeeeee     uuuuullEEEEEEE\tGerehw s suilbupEPPPPPPPP\n",
      "nncccEEEEEEEaaaaaaaaaaaaa\tGnicEPPPPPPPPPPPPPPPPPPPP\n",
      "eeeeeeeuuuuuuuddddddddddo\tGereeh etiuq dednuofnocEP\n",
      "teeEEEEEEEEEEEEEEEEEEEEEE\tGtemEPPPPPPPPPPPPPPPPPPPP\n",
      "dnnnaaasss    u          \tGdnats tsaf rehtegotEPPPP\n",
      "ekuuuuu           dddnnnn\tGeklat ton fo gnidnatsEPP\n",
      "ssuuuuuuuuuuu   uuuuuuuuu\tGsuilbup doog ereehcEPPPP\n",
      "isssiillEEEEEEEEEEEEEEEEE\tGissacEPPPPPPPPPPPPPPPPPP\n",
      "dncc uuuuuu   uuuuuuuuull\tGdna euael sv suilbupEPPP\n",
      "=========================\t=========================\n",
      "Average loss at step 21000: 0.000138\n",
      "=========================\t=========================\n",
      "tshhoEEEEEEEEEEEEEEEEEEEE\tGtsohgEPPPPPPPPPPPPPPPPPP\n",
      "ob          uuuuuuuuuuuuu\tGot llet eeht uohtEPPPPPP\n",
      "turbbbEEEEEEEEEEEEEEEEEEE\tGturbEPPPPPPPPPPPPPPPPPPP\n",
      "lllll              uueeee\tGllew neht i llahs eesEPP\n",
      "tshhoEEEEEEEEEEEEEEEEEEEE\tGtsohgEPPPPPPPPPPPPPPPPPP\n",
      "ii     iiiiiiiiiiiiiiiiii\tGi ta ippilihpEPPPPPPPPPP\n",
      "turbbbEEEEEEEEEEEEEEEEEEE\tGturbEPPPPPPPPPPPPPPPPPPP\n",
      "yhw                   EEE\tGyhw i lliw ees eeht taEP\n",
      "llllliiiii         lEEEEE\tGlli tirips i dluowEPPPPP\n",
      "yoo  uuuuuuuuuuuuuuuuuuuu\tGyob suicul surravEPPPPPP\n",
      "cuullEEEEEEEEEEEEEEEEEEEE\tGculEPPPPPPPPPPPPPPPPPPPP\n",
      "eeee tgtss         EEEEEE\tGeht sgnirts ym drolEPPPP\n",
      "urbbEEEEEEEEEEEEEEEEEEEEE\tGurbEPPPPPPPPPPPPPPPPPPPP\n",
      "ee  sssssss             E\tGeh sekniht eh llits siEP\n",
      "suuuuulllluuulllllllEEEEE\tGsuicul ekawaEPPPPPPPPPPP\n",
      "cuullEEEEEEEEEEEEEEEEEEEE\tGculEPPPPPPPPPPPPPPPPPPPP\n",
      "ymm  bbEEEEEEEEEEEEEEEEEE\tGym drolEPPPPPPPPPPPPPPPP\n",
      "urbbEEEEEEEEEEEEEEEEEEEEE\tGurbEPPPPPPPPPPPPPPPPPPPP\n",
      "dii           uuuEEEEEEEE\tGdid ts uoht emaerdEPPPPP\n",
      "cuullEEEEEEEEEEEEEEEEEEEE\tGculEPPPPPPPPPPPPPPPPPPPP\n",
      "ym                    EEE\tGym drol i od ton wonkEPP\n",
      "urbbEEEEEEEEEEEEEEEEEEEEE\tGurbEPPPPPPPPPPPPPPPPPPPP\n",
      "seus  tttttt         EEEE\tGsey taht uoht did tsEPPP\n",
      "cuullEEEEEEEEEEEEEEEEEEEE\tGculEPPPPPPPPPPPPPPPPPPPP\n",
      "gnnnnnnn   EEEEEEEEEEEEEE\tGgnihton ym drolEPPPPPPPP\n",
      "=========================\t=========================\n",
      "Average loss at step 24000: 0.000110\n",
      "=========================\t=========================\n",
      "suuuuuuuuuuuu        EEEE\tGsuturb uoht peels tsEPPP\n",
      "suuuuuuuuuuuu        EEEE\tGsuturb uoht peels tsEPPP\n",
      "hcucc iiiiiiiiiiiiiiiiiaa\tGhcus snoitagitsni euahEP\n",
      "taaa  uuuEEEEEEEEEEEEEEEE\tGtahw emorEPPPPPPPPPPPPPP\n",
      "ymm sssuuuucaaaaaaccaaoEE\tGym srotsecna did morfEPP\n",
      "eeeeeuuuuuuuuuuuuuuueeeee\tGekaeps ekirts esserderEP\n",
      "maa      ddduu   uuEEEEEE\tGma i detaertne otEPPPPPP\n",
      "o              uueeeeeeee\tGo emor i ekam eehtEPPPPP\n",
      "reeeeuuuuuuuulEEEEEEEEEll\tGretne suiculEPPPPPPPPPPP\n",
      "cuullEEEEEEEEEEEEEEEEEEEE\tGculEPPPPPPPPPPPPPPPPPPPP\n",
      "rraaaccccccc     ssssssEE\tGris hcram si detsawEPPPP\n",
      "ekuuuuucccccnccccEEEEEEEE\tGekconk nihtiwEPPPPPPPPPP\n",
      "turbbbEEEEEEEEEEEEEEEEEEE\tGturbEPPPPPPPPPPPPPPPPPPP\n",
      "sii    ooEEEEEEEEEEEEEEEE\tGsit doogEPPPPPPPPPPPPPPP\n",
      "oo              uueeEEEEE\tGog ot eht etag emosEPPPP\n",
      "eeeeeeeeee     ggtttt EEE\tGeneewteb eht gnitca foEP\n",
      "reeeeuuuuuuuulEEEEEEEEEll\tGretne suiculEPPPPPPPPPPP\n",
      "cuullEEEEEEEEEEEEEEEEEEEE\tGculEPPPPPPPPPPPPPPPPPPPP\n",
      "rroii         bbbbbbbbbbb\tGris sit ruoy rehtorbEPPP\n",
      "turbbbEEEEEEEEEEEEEEEEEEE\tGturbEPPPPPPPPPPPPPPPPPPP\n",
      "sss     uuueEEEElllllllll\tGsi eh enolaEPPPPPPPPPPPP\n",
      "cuullEEEEEEEEEEEEEEEEEEEE\tGculEPPPPPPPPPPPPPPPPPPPP\n",
      "ooo              uuuuuuuE\tGon ris ereht era eomEPPP\n",
      "turbbbEEEEEEEEEEEEEEEEEEE\tGturbEPPPPPPPPPPPPPPPPPPP\n",
      "eoouu              EEEEEE\tGeod uoy wonk mehtEPPPPPP\n",
      "=========================\t=========================\n",
      "Average loss at step 27000: 0.000091\n",
      "=========================\t=========================\n",
      "EEEEEEEEEEEEEEEEEEEEEEEEE\tGEPPPPPPPPPPPPPPPPPPPPPPP\n",
      "eeeeuu             ueeees\tGeraet mih ot seceepEPPPP\n",
      "annacccaaaaaaaaaaaaaaaaaa\tGannicEPPPPPPPPPPPPPPPPPP\n",
      "i                    EEEE\tGi ma annic eht teop iEPP\n",
      "EEEEEEEEEEEEEEEEEEEEEEEEE\tGEPPPPPPPPPPPPPPPPPPPPPPP\n",
      "eereeuu               EEE\tGeraet mih rof sih dabEPP\n",
      "nncccEEEEEEEaaaaaaaaaaaaa\tGnicEPPPPPPPPPPPPPPPPPPPP\n",
      "i           ccc    EEEEEE\tGi ma ton annic ehtEPPPPP\n",
      "EEEEEEEEEEEEEEEEEEEEEEEEE\tGEPPPPPPPPPPPPPPPPPPPPPPP\n",
      "tt                 EEEEEE\tGti si on rettam sihEPPPP\n",
      "EEEEEEEEEEEEEEEEEEEEEEEEE\tGEPPPPPPPPPPPPPPPPPPPPPPP\n",
      "eeeeuuu            EEEEEE\tGeraet mih raet mihEPPPPP\n",
      "eeoou    uuuuuuuuuuuuuuue\tGemos ot suiced esuohEPPP\n",
      "tntteeeeeEEEEEEEEEEEEEEEE\tGtnuexeEPPPPPPPPPPPPPPPPP\n",
      "llll      ssssssuulllllll\tGlla eht snaiebelpEPPPPPP\n",
      "suuuuuuuuuuuuuuuuuullllll\tGsutca sutrauqEPPPPPPPPPP\n",
      "rreetttnnnnnnnnuuuuuuuuuo\tGretne ynotna suiuatcoEPP\n",
      "tnacEEEEEEEEEEEEEEEEEEEEE\tGtnaEPPPPPPPPPPPPPPPPPPPP\n",
      "eeeeeuu  nnnnnn     lllll\tGeseht ynam neht llahsEPP\n",
      "ruuubbbbbbbbbbbbb    ssss\tGruoy rehtorb oot tsumEPP\n",
      "pellEEEEEEEEEEEEEEEEEEEEE\tGpelEPPPPPPPPPPPPPPPPPPPP\n",
      "ii     nnnnnnnnnnnnnnnnnn\tGi od tnesnocEPPPPPPPPPPP\n",
      "atcccccEEEllllllllEEEEEEE\tGatcoEPPPPPPPPPPPPPPPPPPP\n",
      "eeciiill      EEEEEEEEEEE\tGekcirp mih enwodEPPPPPPP\n",
      "pellEEEEEEEEEEEEEEEEEEEEE\tGpelEPPPPPPPPPPPPPPPPPPPP\n",
      "=========================\t=========================\n",
      "Average loss at step 30000: 0.000077\n",
      "=========================\t=========================\n",
      "tahhtt  ss         hhhEEE\tGtahw tsod uoht htiwEPPPP\n",
      "uuuu              uuuuuEE\tGuoy ris tahw edart eraEP\n",
      "llollEEEEEEEEEEEEEEEEEEEE\tGlbocEPPPPPPPPPPPPPPPPPPP\n",
      "yluuuuu           ckccccl\tGyleurt ris ni tcepserEPP\n",
      "rrbbEEEEEEEEEEEEEEEEEEEEE\tGrumEPPPPPPPPPPPPPPPPPPPP\n",
      "tuttttthhhh    bbttEEEEEE\tGtub tahw edart traEPPPPP\n",
      "rrreeee    uuuuuuuEEEEEEE\tGrewsna em yltceridEPPPPP\n",
      "boooEEEEEEEEEEEEEEEEEEEEE\tGbocEPPPPPPPPPPPPPPPPPPPP\n",
      "aaaaaaaaa          EEEEEE\tGa edart ris taht iEPPPPP\n",
      "aaaaaa hhhhh   uuuuuoEEEE\tGalf tahw edart uohtEPPPP\n",
      "uuuuuubtttttnnnuuuuuEEEEE\tGuoht ythguan euankEPPPPP\n",
      "llollEEEEEEEEEEEEEEEEEEEE\tGlbocEPPPPPPPPPPPPPPPPPPP\n",
      "yaaa         uuuu    oEEE\tGyan i hceeseb uoy risEPP\n",
      "rrbbEEEEEEEEEEEEEEEEEEEEE\tGrumEPPPPPPPPPPPPPPPPPPPP\n",
      "tahtttnaaaaa        EEEEE\tGtahw naem ts uoht ybEPPP\n",
      "dnre       u      pwwwwEE\tGdnem eem uoht ycwasEPPPP\n",
      "boooEEEEEEEEEEEEEEEEEEEEE\tGbocEPPPPPPPPPPPPPPPPPPPP\n",
      "yhh           uuuuuuuuuuu\tGyhw ris elbboc uoyEPPPPP\n",
      "aaaa               uuuuEE\tGalf uoht tra a relbocEPP\n",
      "boooEEEEEEEEEEEEEEEEEEEEE\tGbocEPPPPPPPPPPPPPPPPPPPP\n",
      "yluuuu              EEEEE\tGylurt ris lla taht iEPPP\n",
      "saaa uuuuuuuuuu    uuuuEE\tGsa reporp nem sa reueEPP\n",
      "aaalllEEEEEEEEEEEEEEEEEEE\tGalfEPPPPPPPPPPPPPPPPPPPP\n",
      "tuuuueeeeeeuu         EEE\tGtub eroferehw tra tonEPP\n",
      "yhw              uuuEEEEE\tGyhw od ts uoht edaelEPPP\n",
      "=========================\t=========================\n",
      "Training finished at: \n",
      "2017-07-03 01:41:55.497446\n"
     ]
    }
   ],
   "source": [
    "num_steps = 30001\n",
    "summary_frequency = 3000\n",
    "\n",
    "#def reverse_text(nb_steps):\n",
    "#    with tf.Session() as session:\n",
    "#        model = create_model()\n",
    "#        tf.initialize_all_variables().run()\n",
    "#        for step in xrange(nb_steps):\n",
    "#            enc_inputs, dec_inputs, weights = get_batch()\n",
    "#            _, loss, _ = model.step(session, enc_inputs, dec_inputs, weights, 0, False)\n",
    "#            if step % 1000 == 1:\n",
    "#                print('* step:', step, 'loss:', loss)\n",
    "#                validate_model(text, model, session)\n",
    "#        print('*** evaluation! loss:', loss)\n",
    "#        validate_model(text, model, session)\n",
    "\n",
    "with tf.Session() as session:\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    tf.initialize_all_variables().run()\n",
    "    for step in range(num_steps):\n",
    "        encoder_input , decoder_input, weights = train_batches_inv.next()\n",
    "        _, loss, _ = model.step(session, encoder_input, decoder_input, weights, 0, False)\n",
    "        mean_loss += loss\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f' % (step, mean_loss))\n",
    "            mean_loss = 0\n",
    "            encoder_inputs, target_decoder_input, _ = train_batches_inv.next()\n",
    "            decoder_inputs = np.ones((bucket_size, batch_size), dtype=np.int32)\n",
    "            decoder_inputs[1:, :] = InvertBatchGenerator._PAD\n",
    "            decoder_inputs[0, : ] = InvertBatchGenerator._GO\n",
    "            \n",
    "            weights = np.zeros((bucket_size, batch_size), dtype=np.float32)\n",
    "            weights[0,:] = 1.0\n",
    "            _, _, output_logits = model.step(session, encoder_inputs, decoder_inputs, weights, \n",
    "                                             bucket_id=0, forward_only=True)\n",
    "            \n",
    "            #Display result\n",
    "            print(\"=========================\\t=========================\")\n",
    "            decoder_out_strs = batches2string_inv(target_decoder_input)\n",
    "            for output_idx in range(len(output_logits)):\n",
    "                output_logits_idx = [x[output_idx] for x in output_logits]\n",
    "                outputs_char = [int(np.argmax(logit)) for logit in output_logits_idx]\n",
    "                out_sentence = ''.join([id2char_inv(c) for c in outputs_char])\n",
    "                out_sentence = out_sentence + '\\t'\n",
    "                out_sentence = out_sentence + decoder_out_strs[output_idx]\n",
    "                print(out_sentence)\n",
    "            print(\"=========================\\t=========================\")\n",
    "\n",
    "print(\"Training finished at: \")\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'seq2seq_model' from 'c:\\\\GitProjects\\\\PythonPlayground\\\\DataScience\\\\Tensorflow\\\\seq2seq_model.py'>"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import imp\n",
    "\n",
    "imp.reload(seq2seq_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-07-03 01:01:27.752803\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#using python nltk english text corpora\n",
    "#write a function to reverse every setenence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow.models.rnn.translate.seq2seq_model as seq2seq_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "num_nodes = 64\n",
    "batch_size = 10\n",
    "\n",
    "def create_model():\n",
    "     return seq2seq_model.Seq2SeqModel(source_vocab_size=vocabulary_size,\n",
    "                                   target_vocab_size=vocabulary_size,\n",
    "                                   buckets=[(word_size + 1, word_size + 2)], # only 1 bucket\n",
    "                                   size=num_nodes,\n",
    "                                   num_layers=3,\n",
    "                                   max_gradient_norm=5.0,\n",
    "                                   batch_size=batch_size,\n",
    "                                   learning_rate=0.5,\n",
    "                                   learning_rate_decay_factor=0.99,\n",
    "                                   use_lstm=True,\n",
    "                                   forward_only=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
