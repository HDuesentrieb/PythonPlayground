---
title: "Practical Machine Learning Peer assessment"
author: "Lai Yiu Ming, Tom"
date: "Sunday, June 14, 2015"
output:
  html_document: default
  pdf_document:
    pandoc_args:
    - +RTS
    - -K32m
    - -RTS
---

#Overview

In this project, our goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. Subjects were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har

The goal of this project is to predict the manner in which subjects did the exercise. This is the "classe" variable in the training set. There are 19622 observations in training set and there are 20 scenarios which the model is applied to deduce in what manner subject did the exercise.

#Introduction

#Data exploration

Training and testing data set are avaiable at cloudfront.net and here is the URL.

Training data set: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

Testing data set: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

```{r cache=TRUE}
library(RCurl)
trainURL <- getURL("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", ssl.verifypeer=FALSE)
training <- read.csv(text = trainURL)
testURL <- getURL("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", ssl.verifypeer=FALSE)
testing <- read.csv(text = testURL)
```

There are 153 numeric attributes which can be used for prediction. 101 of them only available when the new_window=yes. while other 52 attributes always have value. To simplify a bit, only those 52 features are selected when training the model.

```{r cache=TRUE}
selectFeatures <- c(8,9,10,11,37,38,39,40,41,42,43,44,45,46,47,48,49,60,61,62,63,64,65,66,67,68,84,85,86,102,113,114,115,116,117,118,119,120,121,122,123,124,140,151,152,153,154,155,156,157,158,159)
classeIndex <- 160
trainingPCA <- training[, selectFeatures]
for(i in seq_len(ncol(trainingPCA))){
  trainingPCA[, i] <- as.numeric(as.character(trainingPCA[, i]))
}
testingPCA <- testing[, selectFeatures]
```

#Data preprocessing

We investigate the 52 attributes left, and find that there is no missing data. Hence the step of imputing missing values can be skipped.

We apply PCA to further reduce the number of attributes in the observations. There are 25 attributes in the principle component, hence effectively compress the size of the training data into half.

```{r cache=TRUE}
library(caret)
preProc <- preProcess(trainingPCA, method="pca")
trainingPCA <- predict(preProc, trainingPCA)
testingPCA <- predict(preProc, testingPCA)
trainingSet <- cbind(trainingPCA, classe=training[,classeIndex])
```


##Method for building model

There are many methods to build a prediction model. Here are some of them

1. Classification tree

2. Naive Bayes

3. Support vector machine

4. Random forests

In the next section, we will evaluate the above 4 models using cross validation to find out which type of model is best fit for this task.

##Cross validation

In this section, K-fold (K=10) is used to evaluate the 4 models in the previous section. In order to reduce the amount of time in selecting model. a subset of data (around 2000 observation) is sampled (without replacement) from the full dataset.

##Sample 2000 observations from dataset

```{r cache=TRUE}
samples <- sample(19622, 2000, replace=FALSE)
samplesTraining <- trainingSet[samples,]
```

##Model Comparison

In this section, we are going to compare each of the models under K-folds. The best algorithm should have the greatest accuracy, and perform consistently.

```{r cache=TRUE, message=F, warning=F}
library(e1071)
set.seed(32323)
folds <- createFolds(y=samplesTraining[, 26], k=10, list=TRUE, returnTrain=TRUE)
resultRF <- c(); resultNB <- c(); resultRpart <- c(); resultSVM <- c()
for(i in 1:10){
  foldTrain <- samplesTraining[unlist(folds[i]),]
  foldTest <- samplesTraining[-unlist(folds[i]),]
  modelRF <- train(y=as.factor(foldTrain[, 26]), 
                   x=foldTrain[, 1:25], 
                   method='rf', prox=TRUE)
  modelNB <- train(y=as.factor(foldTrain[, 26]), 
                   x=foldTrain[, 1:25], 
                   method='nb')
  modelRPART <- train(y=as.factor(foldTrain[, 26]), 
                   x=foldTrain[, 1:25], 
                   method='rpart')
  modelSVM <- svm(foldTrain[, 1:25], as.factor(foldTrain[, 26]))
  predsRF <- predict(modelRF, foldTest[, 1:25])
  predsNB <- predict(modelNB, foldTest[, 1:25])
  predsRPART <- predict(modelRPART, foldTest[, 1:25])
  predsSVM <- predict(modelSVM, foldTest[, 1:25])
  levels(predsRF) <- c('A', 'B', 'C', 'D', 'E'); levels(predsNB) <- c('A', 'B', 'C', 'D', 'E'); 
  levels(predsRPART) <- c('A', 'B', 'C', 'D', 'E'); levels(predsSVM) <- c('A', 'B', 'C', 'D', 'E');
  cm <- table(predsRF,foldTest[,26]);  acc <- (cm[1,1]+cm[2,2]+cm[3,3]+cm[4,4]+cm[5,5])/sum(cm)
  resultRF <- c(resultRF, acc)
  cm <- table(predsNB,foldTest[,26]);  acc <- (cm[1,1]+cm[2,2]+cm[3,3]+cm[4,4]+cm[5,5])/sum(cm)
  resultNB <- c(resultNB, acc)
  cm <- table(predsRPART,foldTest[,26]);  acc <- (cm[1,1]+cm[2,2]+cm[3,3]+cm[4,4]+cm[5,5])/sum(cm)
  resultRpart <- c(resultRpart, acc)
  cm <- table(predsSVM,foldTest[,26]);  acc <- (cm[1,1]+cm[2,2]+cm[3,3]+cm[4,4]+cm[5,5])/sum(cm)
  resultSVM <- c(resultSVM, acc)
}
```

The result is listed as follow

```{r cache=TRUE}
finalResult <- rbind(resultRF, resultNB, resultRpart, resultSVM)
resultMean <- rowMeans(finalResult)
resultSD <- apply(finalResult, 1, sd)
finalResult <- cbind(finalResult, resultMean, resultSD)
colnames(finalResult) <- c("1st", "2nd", "3rd", "4th", "5th", "6th", "7th", "8th", "9th", "10th", "mean", "sd")
finalResult
```

From the result, we find that random forest(ResultRF) and support vector machine(resultSVM) have successful prediction rate over 80%. followed by Naive Bayes(resultNB) with sucessful prediction rate of around 60% and decision tree(resultRpart) perform the worst. We decide to use random forest to build our final prediction model.

#Final prediction model and result

From the above, we have evaluate different models using cross validation. In this section, we are going to create a prediction model with the full training data set.

```{r cache=TRUE}
model.final <- train(y=as.factor(samplesTraining[, 26]), 
                   x=samplesTraining[, 1:25], 
                   method='rf', prox=TRUE)
```

##In sample error and out sample error and validation
Cross validation can help us to remove the models which overfit the training datset, or without sufficient prediction power.

In sample error is the error rate you get on the same data set you used to build your predictor.

```{r cache=TRUE}
predIn <- predict(model.final, samplesTraining[, 1:25])
confusionMatrix(samplesTraining[, 26], predIn)
```

Out sample error is the error rate you get on a new data set.

```{r cache=TRUE}
samples <- sample(19622, 2000, replace=FALSE)
out.sampleFull <- training[samples,]
out.samplePCA <- out.sampleFull [, selectFeatures]
for(i in seq_len(ncol(out.samplePCA))){
  out.samplePCA[, i] <- as.numeric(as.character(out.samplePCA[, i]))
}
out.samplePCA <- predict(preProc, out.samplePCA)
predOut <- predict(model.final, out.samplePCA[, 1:25])
confusionMatrix(out.sampleFull[, classeIndex], predOut)
```

From the above 2 result, we can conclude that the out of sample error is 1-0.8855 = 11.45%.


##Predicting the given 20 test scenarios
```{r cache=TRUE}
predict(model.final, testingPCA)
```

The accuracy is 85%.

#Conclusion
In this project, we present different prediction models. We apply Principle Component Analysis to the training dataset to reduce the number of features for those observations. We have used K-fold to evaluate 4 different prediction model, and according to the result, select the best prediction model for this task. A prediction model is built using randomly selected 2000 observations from the training data set and its performance is reviewed.
